### Import libraries
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier

# Import dataset from Kaggle
import os
os.environ['KAGGLE_USERNAME'] = "vinhnguyen94" # username from the json file
os.environ['KAGGLE_KEY'] = "ee939400b83c8ef989727a70f29fab39" # key from the json file
!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle

# Import dataset on Colab
from google.colab import files
uploaded = files.upload()
import io
train=pd.read_csv(io.BytesIO(uploaded['train.csv']))

# Datasets into Dataframe
On Mac: open new window terminal, slide the data and copy the path
train = pd.read_csv("/Users/vinhnguyen/Desktop/Kaggle/Titanic/train.csv")

train.describe() // train.describe(include = ['O']) if we want only the strings information. include = 'all' is possible

#check the numbers of samples and features
train.shape, test.shape

#Save the 'Id' column
train_ID = train['Id']
test_ID = test['Id']
#Now drop the  'Id' colum since it's unnecessary for  the prediction process.
train.drop("Id", axis = 1, inplace = True)
test.drop("Id", axis = 1, inplace = True)


### Data preprocessing
Select the most important features to check the outliers
# Outliers
fig, ax = plt.subplots()
feature = "TotalBsmtSF"
ax.scatter(x = train[feature], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel(feature, fontsize=13)
plt.show()


Spend time to study the data:
- % of train data
- label distribution
- features correlation
- missing values
- convert strings into floats. Ex: sex, 
- drop, convert and create new features
- observations then decisions
- use pipelines to avoid data leakage (too much preprocessing before split data)

# Plot train Target
import matplotlib.pyplot as plt
import seaborn as sns
sns.countplot(df_train.target)
plt.xlabel('Target')
plt.title('Target distribution')

# Merge 2 dataframes using the same column
new_train = pd.merge(train, text1, on='id', how='inner')

#Heat map to see how features are correlated with the target
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)


# To do data pre-processing on both train and test sets. But WORK ON THE TRAIN FIRST THEN APPLY on TEST
combine = [train,test]
for dataset in combine:
    dataset["Sex"]=dataset["Sex"].map( {"female":1, "male":0}).astype(int)

# Multiple plots
g = sns.FacetGrid(train, col='y') # Initialize the FacetGrid with the data (train) and the target var
g.map(plt.hist, 'X', bins=20) # Call plotting function .map with interval of 20 (for age for ex)

# Correlation between Gender and Target using the mean
train[["Sex","Target"]].groupby(['Sex']).mean().sort_values(by= "Target", ascending = False)

# Count the occurence for each value for categorical feature
train.col1.value_counts()

### MISSING VALUES
# Number of missing values in each column of training data
missing_val_count_by_column = (X_train.isnull().sum())
print(missing_val_count_by_column[missing_val_count_by_column > 0])  # Print only the columns with missing values

# Count the number of NaN per column. If we don't have so much features
len(train) - train.count()

# Get names of columns with missing values
cols_with_missing = [col for col in train.columns
                     if train[col].isnull().any()]

# Complete missing values by the median / "Unknown"
train['Age']=train['Age'].fillna(train['Age'].median() / "Unknown")

# Complete missing values by the most frequent occurence (letter or numerical category). Ex: frequent port of embarkation
freq_port = train.Embarked.dropna().mode()[0]
train["Embarked"] = train["Embarked"].fillna(freq_port)

# Complete missing values by using other correlated features instead of replacing by the mean of the missing variable
# Guess Age values using median values for Age across sets of Pclass and Gender feature combinations
guess_ages = np.zeros((2,3))  # (2,3) cuz Sex is 2 and Pclass is 3

for dataset in combine:
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = dataset[(dataset['Sex'] == i) & \
                                  (dataset['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    dataset['Age'] = dataset['Age'].astype(int)



# Create ranges of age for plotting
train['Age_bins']=pd.cut(x=train['Age'], bins=[0,19, 39, 59,79], labels=['0-10s','20-30s','40-50s','60-70s']) # number of labels should be one less than bins
or
train['AgeBand'] = pd.cut(train['Age'], 5) # Create 5 categories of Age. better. We can add labels if we want. Use qcut if to have balanced categories

train.groupby(["Age_bins"]).mean()
or
train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True) #Wanna know which AgeBand is, on average, more likely to survive

pd.crosstab(train.Age_bins, train.Survived).plot(kind="bar") #Survived = target

# To convert strings into numerical values (for Sex for instance)
for dataset in combine:
    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)


# Convert age into fixed categories of Age
for dataset in combine:    
    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4

# Create another feature called IsAlone. Reminder: combine = [train, test]
for dataset in combine:
    dataset["IsAlone"]=0 #Creation of the new feature
    dataset.loc[dataset["FamilySize"] == 1, "IsAlone"] = 1 # 1 if the person is alone



# Remove columns from the df
train = train.drop(["col1", "col2"], axis=1)


### Modeling
X = train.drop("label", axis = 1)
y = train.label

# split the dataset into training and validation datasets
from sklearn import model_selection
X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.25, random_state=0,
                                                   stratify=y) #random_state to have the same alea and strafify to have a balance in label

# Manually do the split
// train = 80% first rows, then valid = 10% and finally test = 10%. Order matters here
def get_data_splits(df, valid_fraction=0.1):
    valid_fraction = 0.1
    df_srt = df.sort_values('click_time') # Wanna order depending on click_time. Delete if order doesn't matter
    valid_rows = int(len(df_srt) * valid_fraction)
    train = df_srt[:-valid_rows * 2]  # 80%
    # valid size == test size, last two sections of the data
    valid = df_srt[-valid_rows * 2:-valid_rows]  # 10%
    test = df_srt[-valid_rows:]  # 10%
    
    return train, valid, test
    
// to call the function: 
train, valid, test = get_data_splits(df)


# Model building. Following function is used to train a model and to compute the accuracy score
def train_model(classifier, train_vector, train_labels, valid_vector, valid_labels, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(train_vector, train_labels)
    
    # predict the labels on validation dataset
    predictions = classifier.predict(valid_vector)
    
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    
    return metrics.accuracy_score(predictions, valid_labels)
    
# Training with log reg
from sklearn import linear_model
accuracy = train_model(linear_model.LogisticRegression(random_state=0, penalty = "l2", solver='newton-cg', max_iter =200, C=1.6), X_train, y_train, X_valid, y_valid)
print ("LR : ", accuracy) 

### Tuning hyperparam
# Gridsearch for log reg
from sklearn import model_selection
param_grid = {'C': [0.1,1,10],
              'solver': ['newton-cg', 'saga', 'lbfgs']
              } 
 
clf = model_selection.GridSearchCV(linear_model.LogisticRegression(max_iter=300), param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)
best_clf = clf.fit(X_train, y_train)
clf.best_params_ #to show best params found

# Predict on test
y_pred = best_model.predict(X_test)

# Output creation
test["Prediction"] = y_pred

# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': X_test.index,
                       'Target': y_pred})
output.to_csv('submission.csv', index=False)

# Submission
test.to_csv('/Users/vinhnguyen/Desktop/Kaggle/Titanic/submission.csv', index=False)
