# Import dataset from Kaggle
import os
os.environ['KAGGLE_USERNAME'] = "vinhnguyen94" # username from the json file
os.environ['KAGGLE_KEY'] = "ee939400b83c8ef989727a70f29fab39" # key from the json file
!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle

# Datasets into Dataframe
On Mac: open new window terminal, slide the data and copy the path
train = pd.read_csv("train.csv")

train.head()
train.describe() // train.describe(include = ['O']) if we want only the strings information. include = 'all' is possible

Spend time to study the data:
- % of train data
- label distribution
- features correlation
- convert strings into floats
- we have to drop, classify and create features
- observations then decisions


# Merge 2 dataframes using the same column
new_train = pd.merge(train, text1, on='id', how='inner')

# Count the number of NaN per column
len(train) - train.count()

# Plot train labels
import matplotlib.pyplot as plt
import seaborn as sns
sns.countplot(df_train.label)
plt.xlabel('Label')
plt.title('Label distribution')

# Multiple plots
g = sns.FacetGrid(train, col='y') # Initialize the FacetGrid with the data (train) and the target var
g.map(plt.hist, 'X', bins=20) # Call plotting function .map with interval of 20 (for age for ex)

# Correlation between Gender and Target using the mean
train[["Sex","Target"]].groupby(['Sex']).mean().sort_values(by= "Target", ascending = False)

# Create ranges of age for plotting
train['Age_bins']=pd.cut(x=train['Age'], bins=[0,19, 39, 59,79], labels=['0-10s','20-30s','40-50s','60-70s']) # number of labels should be one less than bins
train.groupby(["Age_bins"]).mean()
pd.crosstab(train.Age_bins, train.Survived).plot(kind="bar") #Survived = target




# split the dataset into training and validation datasets
from sklearn import model_selection
X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.25, random_state=0,
                                                   stratify=y) #random_state to have the same alea and strafify to have a balance in label

# Model building. Following function is used to train a model and to compute the accuracy score
def train_model(classifier, train_vector, train_labels, valid_vector, valid_labels, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(train_vector, train_labels)
    
    # predict the labels on validation dataset
    predictions = classifier.predict(valid_vector)
    
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    
    return metrics.accuracy_score(predictions, valid_labels)
    
# Training with log reg
from sklearn import linear_model
accuracy = train_model(linear_model.LogisticRegression(random_state=0, penalty = "l2", solver='newton-cg', max_iter =200, C=1.6), X_train, y_train, X_valid, y_valid)
print ("LR : ", accuracy) 

# Gridsearch for log reg
from sklearn import model_selection
param_grid = {'C': [0.1,1,10],
              'solver': ['newton-cg', 'saga', 'lbfgs']
              } 
 
clf = model_selection.GridSearchCV(linear_model.LogisticRegression(), param_grid = param_grid, cv = 2, verbose=True, n_jobs=-1)

# Fit on train and predict on test
best_model = clf.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
