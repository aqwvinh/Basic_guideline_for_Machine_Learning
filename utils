Spend time to study the data. Here are some advice:
- check the % that train data represent
- plot label distribution
- check features correlation
- check missing values
- convert strings into floats. Ex: gender, class etc 
- drop, convert and create new features --> feature engineering
- think as "observations then decisions"
- use pipelines to avoid data leakage (too much preprocessing before split data)

STEPS TO FOLLOW FOR CLASSICAL TASKS
### Import libraries
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier

### Import dataset 
# from Kaggle
import os
os.environ['KAGGLE_USERNAME'] = "vinhnguyen94" # username from the json file
os.environ['KAGGLE_KEY'] = "ee939400b83c8ef989727a70f29fab39" # key from the json file. Do it each time
!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle
train = pd.read_csv("train.csv")

# on Colab
from google.colab import files
uploaded = files.upload()
import io
train=pd.read_csv(io.BytesIO(uploaded['train.csv']))

# on Jupyter
On Mac: open new window terminal, slide the data and copy the path
train = pd.read_csv("/Users/vinhnguyen/Desktop/Kaggle/Titanic/train.csv")

train.describe() // train.describe(include = ['O']) if we want only the strings information. include = 'all' is possible

# Check if the target variable is normal, sometimes there can be very bad mistake that would ruin the model (like a negative price)

#check the numbers of samples and features
train.shape, test.shape

#Save the 'Id' column
train_ID = train['Id']
test_ID = test['Id']
#Now drop the  'Id' colum since it's unnecessary for the prediction process.
train.drop("Id", axis = 1, inplace = True)
test.drop("Id", axis = 1, inplace = True)

### Understand the data
Select the most important features to check the outliers and to check multicorrelation (reduce nb of features)
# Heatmap to see how features are correlated
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)

# Heatmap to check the features that contribute the most to the target variable
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'Target')['Target'].index
cm = np.corrcoef(train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

# Plot train Target
import matplotlib.pyplot as plt
import seaborn as sns
sns.countplot(df_train.target)
plt.xlabel('Target')
plt.title('Target distribution')

# Spot outliers. Strategy: delete very bad outliers. not all outliers to have a model robust on outliers
fig, ax = plt.subplots()
feature = "TotalBsmtSF"
ax.scatter(x = train[feature], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel(feature, fontsize=13)
plt.show()
# Drop chosen outliers
train = train.drop(train[(train['TotalBsmtSF']>4000) & (train['SalePrice']<300000)].index)  # Example for the house pricing

# Multiple plots TO DO
g = sns.FacetGrid(train, col='y') # Initialize the FacetGrid with the data (train) and the target var
g.map(plt.hist, 'X', bins=20) # Call plotting function .map with interval of 20 (for age for ex)

# Correlation between Gender and Target using the mean
train[["Sex","Target"]].groupby(['Sex']).mean().sort_values(by= "Target", ascending = False)

# Merge 2 dataframes using the same column
new_train = pd.merge(train, text1, on='id', how='inner')

### DATA PREPROCESSING
To do data pre-processing on both train and test sets. But WORK ON THE TRAIN FIRST THEN APPLY on TEST

# Concatenate train and test data to do the preprocessing once. Not the best strategy but faster
ntrain = train.shape[0] # Essential to re-split the data before modeling. shape[0] to have only the nb of rows
ntest = test.shape[0]
y_train = train.target.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['target'], axis=1, inplace=True)
print("all_data size is : {}".format(all_data.shape))


### MISSING VALUES
# Print only the columns with missing values and there corresponding %
total_miss = all_data.isnull().sum().sort_values(ascending=False)
percent_miss = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_miss, percent_miss], axis=1, keys=['Total_miss', 'Percent_miss'])
// Strategy: When lots of features, we can delete the features with high % of missing values (let's say 15%). Check if these variables are important

# (Variante) Print only the columns with missing values
missing_val_count_by_column = (df.isnull().sum())
print(missing_val_count_by_column[missing_val_count_by_column > 0])  # Print only the columns with missing values

# (Variante) Count the number of NaN per column. If we don't have so much features
len(train) - train.count()

# Get names of columns with missing values
cols_with_missing = [col for col in train.columns
                     if train[col].isnull().any()]

# Get columns for categorical features (replace by the most frequent occurence) and numerical features (replace by the mean)
categorical_cols = [cname for cname in all_data.columns if
                    all_data[cname].nunique() < 10 and    #to limit the size of the encoding
                    all_data[cname].dtype == "object" and
                    all_data[cname].isnull().sum() > 0
                    ]
numerical_cols = [cname for cname in all_data.columns if 
                all_data[cname].dtype in ['int64', 'float64'] and 
                all_data[cname].isnull().sum() > 0
                ]
# Replace by the mean and the most frequent values
for feature in categorical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].dropna().mode()[0])

for feature in numerical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].mean())
  
# (Variante) Complete missing values by the median / "Unknown"
train['Age']=train['Age'].fillna(train['Age'].median() / "Unknown")

# (Variante) Complete missing values by the most frequent occurence (letter or numerical category). Ex: frequent port of embarkation
freq_port = train.Embarked.dropna().mode()[0]
train["Embarked"] = train["Embarked"].fillna(freq_port)

# (Variante) Complete missing values by using other correlated features instead of replacing by the mean of the missing variable
# Guess Age values using median values for Age across sets of Pclass and Gender feature combinations
guess_ages = np.zeros((2,3))  # (2,3) cuz Sex is 2 and Pclass is 3
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = df[(df['Sex'] == i) & \
                                  (df['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    df['Age'] = df['Age'].astype(int)
    
    
### Categorizing
# Create ranges of age for plotting
train['Age_bins']=pd.cut(x=train['Age'], bins=[0,19, 39, 59,79], labels=['0-10s','20-30s','40-50s','60-70s']) # number of labels should be one less than bins
or (BETTER)
train['AgeBand'] = pd.cut(train['Age'], 5) # Create 5 categories of Age. We can add labels if we want. Use qcut if to have balanced categories

# Group the AgeBand depending on the target variable
train.groupby(["Age_bins"]).mean()
or
train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)

pd.crosstab(train.Age_bins, train.Survived).plot(kind="bar") #Survived = target

# Convert age into fixed categories of Age after the pd.cut   
dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
dataset.loc[ dataset['Age'] > 64, 'Age'] = 4


### Creation of new feature. Some ideas
# TITANIC. Create another feature called IsAlone. Reminder: combine = [train, test]
for dataset in combine:
    dataset["IsAlone"]=0 #Creation of the new feature
    dataset.loc[dataset["FamilySize"] == 1, "IsAlone"] = 1 # 1 if the person is alone
    
# HOUSE PRICING. Create a variable for the total size of the house. Try to group correlated variables
all_data["tot_surface"] = all_data["TotalBsmtSF"] + all_data["1stFlrSF"] + all_data["2ndFlrSF"]

### TO GO FURTHER: CHECK NORMALITY. The variables should be normal
from scipy import stats
#histogram and normal probability plot
sns.distplot(y_train);
fig = plt.figure()
res = stats.probplot(y_train, plot=plt) #The variable has to follow the red line (that represents the normal distribution).
# In case of hyperbol (positive skewness), apply log
y_train = np.10log(y_train)   #Recheck now. Be careful on the submission, we need to inverse
and don't forget: y_pred = 10**y_pred

### Encoding
BEST PRACTICE : MANUALLY ENCODE WHEN ORDER MATTERS AND ONE-HOT ENCODE WHEN NO HIERARCHY

# Converting numeric features to categorical features. Especially for the year or month --> need to create categories
from sklearn.preprocessing import LabelEncoder
str_cols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass']
for col in str_cols:
    all_data[col] = all_data[col].astype(str)
    label = LabelEncoder()
    arr = label.fit_transform(all_data[col])
    all_data[col] = arr

# Count the occurence for each value for categorical feature
train.col1.value_counts()

# Convert string categorical vars into numerical ordered manually. Important to do that manually cuz order matters (Ordinal encoding)
all_data['GarageQual'].unique()
all_data["GarageQual"]=all_data["GaraQual"].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}).astype(int)
# One-hot encoding for categorical features using pd.dummies
cat_df = all_data.select_dtypes(include=[np.object])  #Take all the categorical features (strings)
for col in cat_df.columns:
    cat_df = pd.concat([cat_df, pd.get_dummies(cat_df[col], drop_first=True)], axis=1)
    cat_df.drop(col, axis=1, inplace=True)
data_encoded = pd.concat([all_data.select_dtypes(include=[np.number]),     # Concat with the numerical features
    cat_df.drop(cat_df.select_dtypes(include=[np.object]), axis=1)], axis=1)   # Don't keep the original categorical features
data_encoded.head(2)

# One-hot encoding using pandas (easier)
all_data = pd.get_dummies(all_data)

# Encode with LabelEncoder. THE DIFFERENCE BETWEEN ONE-HOT AND LABEL ENCODING IS THAT ONE-HOT DOESNT HAVE HIERARCHY. 
from sklearn.preprocessing import LabelEncoder
# Select all cat feature to encode: variables with categories and a hierarchy 
cat_features = ['MSSubClass', 'MSZoning', 'Street',
       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',
       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',
       'OverallQual', 'OverallCond', 'RoofStyle',
       'RoofMatl', 'MasVnrType',
       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',
       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
       'Heating', 'HeatingQC',
       'CentralAir', 'Electrical',
       'KitchenQual',
       'Functional', 'Fireplaces', 'GarageType', 'GarageFinish',
      'GarageQual', 'GarageCond', 'PavedDrive',
       'SaleType',
       'SaleCondition']

for feature in cat_features:
  enc = LabelEncoder()
  all_data[feature] = enc.fit_transform(all_data[feature])   # Not the best way but i've to find better one. Not good to fit the encoder on test



### Modeling
# Importing the models
from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor
from sklearn.svm import LinearSVR, SVR

# Split train and test sets
train = all_data[:ntrain]
test = all_data[ntrain:]

# Summarize the X and y variables. Reminder: y_train can be normalized by the log function
X = train.drop("label", axis = 1)
y = train.label   # check if the target variable is normalized: y_train = np.log(y_train)
X_test = test
X.shape, y.shape, X_test.shape  # To check if the shapes are correct

When the dataset is big enough, no need to do cross-validation so we can split into train and valid
# split the dataset into training and validation datasets
from sklearn import model_selection
X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.25, random_state=0,
                                                   stratify=y) #random_state to have the same alea and strafify to have a balance in label

# Manually do the split
// train = 80% first rows, then valid = 10% and finally test = 10%. Order matters here
def get_data_splits(df, valid_fraction=0.1):
    valid_fraction = 0.1
    df_srt = df.sort_values('click_time') # Wanna order depending on click_time. Delete if order doesn't matter
    valid_rows = int(len(df_srt) * valid_fraction)
    train = df_srt[:-valid_rows * 2]  # 80%
    # valid size == test size, last two sections of the data
    valid = df_srt[-valid_rows * 2:-valid_rows]  # 10%
    test = df_srt[-valid_rows:]  # 10%
    
    return train, valid, test
    
// to call the function: 
train, valid, test = get_data_splits(df)

# Using Robust Scaler to transform X_train. Aim: scale features to be robust to outliers
from sklearn.preprocessing import RobustScaler
robust_scaler = RobustScaler()
X_train_scaled = robust_scaler.fit_transform(X)
X_test_scaled = robust_scaler.transform(test)

# Creating the models. ADAPT ACCORDING THE TASK. HERE IS HOUSE PRICING PREDICTION
models = [LinearRegression(), SVR(), SGDRegressor(), SGDRegressor(max_iter=1000, tol=1e-3), GradientBoostingRegressor(), RandomForestRegressor(),
             Lasso(), Lasso(alpha=0.01, max_iter=10000), Ridge(), BayesianRidge(), KernelRidge(), KernelRidge(alpha=0.6, kernel='polynomial',degree=2, coef0=2.5),
             ElasticNet(), ElasticNet(alpha=0.001, max_iter=10000), ExtraTreesRegressor()]
names = ['Linear Regression','Support Vector Regression','Stochastic Gradient Descent','Stochastic Gradient Descent 2','Gradient Boosting Tree','Random Forest',
         'Lasso Regression','Lasso Regression 2','Ridge Regression','Bayesian Ridge Regression','Kernel Ridge Regression','Kernel Ridge Regression 2',
         'Elastic Net Regularization','Elastic Net Regularization 2','Extra Trees Regression']

# Define a root mean square error function
from sklearn.model_selection import KFold, cross_val_score
def rmse(model, X, y):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse

# Perform 5-folds cross-validation to evaluate the models 
for model, name in zip(models, names):
    # Root mean square error
    score = rmse(model, X_train_scaled, y)
    print(f"- {name}: Mean: {round(score.mean(),3)}, Std: {round(score.std(),3)}")


# (Variante) Model building. Following function is used to train a model and to compute the accuracy score
def train_model(classifier, train_vector, train_labels, valid_vector, valid_labels, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(train_vector, train_labels)
    
    # predict the labels on validation dataset
    predictions = classifier.predict(valid_vector)
    
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    
    return metrics.accuracy_score(predictions, valid_labels)
    
# Training with log reg
from sklearn import linear_model
accuracy = train_model(linear_model.LogisticRegression(random_state=0, penalty = "l2", solver='newton-cg', max_iter =200, C=1.6), X_train, y_train, X_valid, y_valid)
print ("LR : ", accuracy) 

### Tuning hyperparam
# Gridsearch for log reg
from sklearn import model_selection
param_grid = {'C': [0.1,1,10],
              'solver': ['newton-cg', 'saga', 'lbfgs']
              } 
 
clf = model_selection.GridSearchCV(linear_model.LogisticRegression(max_iter=300), param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)
best_clf = clf.fit(X_train, y_train)
clf.best_params_ #to show best params found

### Final model
best_model = ExtraTreesRegressor()  # For example with the best_params found earlier if applicable
best_model.fit(X_train_scaled, y_train)

# Predict on test
y_pred = best_model.predict(X_test_scaled)
or
y_pred = np.exp(best_model.predict(X_test_scaled))  # If we've applied log to normalize the variables

# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': X_test.index,
                       'Target': y_pred})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")

# Submission on Jupyter if we wanna select the file
test.to_csv('/Users/vinhnguyen/Desktop/Kaggle/Titanic/submission.csv', index=False)
