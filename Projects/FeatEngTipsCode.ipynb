{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Features Engineering Codes for fast coding</h1> <center>\n",
    "    <center> <h2> Auteur: Vinh NGUYEN </h2> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Rare labels](#Rare-labels)\n",
    "* [Missing Indicator](#Missing-indicator)\n",
    "* [MV Imputer](#MV-Imputer)\n",
    "    * [SimpleImputer for mean/median](#SimpleImputer-for-mean/median)\n",
    "    * [Same technique for arbitrary imputation. Strategy = 'constant'](#Same-technique-for-arbitrary-imputation.-Strategy-=-'constant')\n",
    "    * [Same for most_frequent. We can use most_frequent strat for both num and cat variables](#Same-for-most_frequent.-We-can-use-most_frequent-strat-for-both-num-and-cat-variables)\n",
    "    * [Automatic best imputation method search](#Automatic-best-imputation-method-search)\n",
    "* [FeatureEngine](#FeatureEngine)\n",
    "    * [Using pipeline from sklearn](#Using-pipeline-from-sklearn)\n",
    "    * [Arbitrary method](#Arbitrary-method)\n",
    "    * [EndTail method](#EndTail-method)\n",
    "    * [Add MissingIndicator](#Add-MissingIndicator)\n",
    "* [Encoding](#Encoding)\n",
    "    * [One hot encoding](#One-hot-encoding)\n",
    "    * [One-hot encoding of top categories](#One-hot-encoding-of-top-categories)\n",
    "    * [Ordinal encoding](#Ordinal-encoding)\n",
    "    * [Count/Frequency encoding](#Count/Frequency-encoding)\n",
    "    * [Target guided encoding](#Target-guided-encoding)\n",
    "    * [Mean encoding](#Mean-encoding)\n",
    "    * [Probability ratio encoding](#Probability-ratio-encoding)\n",
    "    * [Weight of evidence encoding](#Weight-of-evidence-encoding)\n",
    "    * [Compare encoding methods performance on RF](#Compare-encoding-methods-performance-on-RF)\n",
    "* [Variable transformation](#Variable-transformation)\n",
    "* [Discretisation](#Discretisation)\n",
    "* [Outliers handling: only remove from train set](#Outliers-handling:-only-remove-from-train-set)\n",
    "* [Features scaling: important except for trees](#Features-scaling:-important-except-for-trees)\n",
    "* [Mixed variables](#Mixed-variables)\n",
    "* [Date and time variables](#Date-and-time-variables)\n",
    "    * [Date](#Date)\n",
    "    * [Time](#Time)\n",
    "* [Recapitulation: Pipeline](#Recapitulation:-Pipeline)\n",
    "    * [Add Cross-Validation](#Add-cross-validation)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pb : Overfitting due to the split train/test\n",
    "<br> **Solution : Groupe rare labels under a new label \"rare\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION TO CALCULATE:\n",
    "# 1) the % of houses per category (var)\n",
    "# 2) the mean target per category\n",
    "\n",
    "\n",
    "def calculate_mean_target_per_category(df, var):\n",
    "\n",
    "    # total number of houses\n",
    "    total_houses = len(df)\n",
    "\n",
    "    # percentage of houses per category\n",
    "    temp_df = pd.Series(df[var].value_counts() / total_houses).reset_index()\n",
    "    temp_df.columns = [var, 'perc_houses']\n",
    "\n",
    "    # add the mean target\n",
    "    temp_df = temp_df.merge(df.groupby([var])['target'].mean().reset_index(),\n",
    "                            on=var,\n",
    "                            how='left')\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "#apply : temp_df = calculate_mean_target_per_category(data, 'Neighborhood')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO PLOT THE CATEGORY FREQUENCY AND THE MEAN TARGET\n",
    "# This will help us visualise the relationship between the\n",
    "# target and the labels of the  categorical variable\n",
    "\n",
    "def plot_categories(df, var):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    plt.xticks(df.index, df[var], rotation=90)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax.bar(df.index, df[\"perc_houses\"], color='lightgrey')\n",
    "    ax2.plot(df.index, df[\"target\"], color='green', label='Seconds')\n",
    "    ax.axhline(y=0.05, color='red')\n",
    "    ax.set_ylabel('percentage of houses per category')\n",
    "    ax.set_xlabel(var)\n",
    "    ax2.set_ylabel('Average target per category')\n",
    "    plt.show()\n",
    "    \n",
    "#apply : plot_categories(temp_df, 'Neighborhood')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO REPLACE ALL THE LABELS THAT APPEAR IN <5% BY THE LABEL \"RARE\"\n",
    "\n",
    "def group_rare_labels(df, var):\n",
    "\n",
    "    total_houses = len(df)\n",
    "\n",
    "    # first I calculate the % of houses for each category\n",
    "    temp_df = pd.Series(df[var].value_counts() / total_houses)\n",
    "\n",
    "    # now I create a dictionary to replace the rare labels with the\n",
    "    # string 'rare' if they are present in less than 5% of houses\n",
    "\n",
    "    grouping_dict = {\n",
    "        k: ('rare' if k not in temp_df[temp_df >= 0.05].index else k)\n",
    "        for k in temp_df.index\n",
    "    }\n",
    "\n",
    "    # now I replace the rare categories\n",
    "    tmp = df[var].map(grouping_dict)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "# apply : # group rare labels in Neighborhood\n",
    "\n",
    "#data['Neighborhood_grouped'] = group_rare_labels(data, 'Neighborhood')\n",
    "\n",
    "#data[['Neighborhood', 'Neighborhood_grouped']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a binary missing indicator may help improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use the following variables,\n",
    "# some are categorical some are numerical\n",
    "\n",
    "cols_to_use = [\n",
    "    'LotFrontage', 'MasVnrArea', # numerical\n",
    "    'BsmtQual', 'FireplaceQu', # categorical\n",
    "    'SalePrice' # target\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function to add a missing indicator\n",
    "# binary variable\n",
    "\n",
    "def missing_indicator(df, variable):    \n",
    "    return np.where(df[variable].isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's loop over all the variables and add a binary \n",
    "# missing indicator with the function we created\n",
    "\n",
    "for variable in cols_to_use:\n",
    "    X_train[variable+'_NA'] = missing_indicator(X_train, variable)\n",
    "    X_test[variable+'_NA'] = missing_indicator(X_test, variable)\n",
    "    \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MV Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer for mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to make lists, indicating which features\n",
    "# will be imputed with each method\n",
    "\n",
    "numeric_features_mean = ['LotFrontage']\n",
    "numeric_features_median = ['MasVnrArea', 'GarageYrBlt']\n",
    "\n",
    "# then we instantiate the imputers, within a pipeline\n",
    "# we create one mean imputer and one median imputer\n",
    "# by changing the parameter in the strategy\n",
    "\n",
    "numeric_mean_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "numeric_median_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "# then we put the features list and the transformers together\n",
    "# using the column transformer\n",
    "\n",
    "# we need to add remainder = True to indicate what we want\n",
    "# ALL the columns returned at the end of the transformation\n",
    "# and not just the engineered ones, which is the default\n",
    "# behaviour of ColumnTransformer. \n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('mean_imputer', numeric_mean_imputer, numeric_features_mean),\n",
    "    ('median_imputer', numeric_median_imputer, numeric_features_median)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# parameters of the ColumnTransformer\n",
    "# remainder = 'passthrough' indicates that we want to retain ALL the columns in the dataset\n",
    "            # otherwise only those specified in the imputing steps will be kept\n",
    "    \n",
    "# for more details follow the sklearn page:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we fit the preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# and now we can impute the data\n",
    "X_train = preprocessor.transform(X_train)\n",
    "\n",
    "# and check it worked\n",
    "np.mean(np.isnan(X_train))\n",
    "\n",
    "# and now we can impute the test data\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# and check it worked\n",
    "np.mean(np.isnan(X_test))\n",
    "\n",
    "# remember that the returned object  is a NumPy array\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's capture the columns in a list\n",
    "\n",
    "remainder_cols = [cols_to_use[c] for c in [0, 1, 2, 3, 4, 5]]\n",
    "remainder_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the data back in a dataframe\n",
    "pd.DataFrame(X_train,\n",
    "             columns = numeric_features_mean+numeric_features_median+remainder_cols).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same technique for arbitrary imputation. Strategy = 'constant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to make lists, indicating which features\n",
    "# will be imputed with each value\n",
    "\n",
    "features_LotFrontAge = ['LotFrontage']\n",
    "features_MasVnrArea = ['MasVnrArea']\n",
    "features_GarageYrBlt = ['GarageYrBlt']\n",
    "\n",
    "# then we instantiate the imputers, within a pipeline\n",
    "# we create one imputer per feature\n",
    "# within the imputer I indicate the arbitrary value\n",
    "# which is differet for each variable\n",
    "\n",
    "imputer_LotFrontAge = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 999)),\n",
    "])\n",
    "\n",
    "imputer_MasVnrArea = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = -10)),\n",
    "])\n",
    "\n",
    "imputer_GarageYrBlt = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 1700)),\n",
    "])\n",
    "\n",
    "# then we put the features list and the transformers together\n",
    "# using the column transformer\n",
    "\n",
    "# in this example, I will use the default parameter of ColumnTransformer\n",
    "# remainder = drop, which means that only the imputed features will\n",
    "# be retained, and the rest dropped\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('imputer_LotFrontAge', imputer_LotFrontAge, features_LotFrontAge),\n",
    "    ('imputer_MasVnrArea', imputer_MasVnrArea, features_MasVnrArea),\n",
    "    ('imputer_GarageYrBlt', imputer_GarageYrBlt, features_GarageYrBlt)\n",
    "],remainder = 'drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for most_frequent. We can use most_frequent strat for both num and cat variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to make lists, indicating which features\n",
    "# will be imputed with each method\n",
    "\n",
    "features_numeric = ['BsmtUnfSF', 'LotFrontage', 'MasVnrArea', ] \n",
    "features_categoric = ['BsmtQual', 'FireplaceQu', 'MSZoning',\n",
    "                      'Street', 'Alley']\n",
    "\n",
    "# then we instantiate the imputers, within a pipeline\n",
    "# we create one mean imputer and one frequent category imputer\n",
    "# by changing the parameter in the strategy\n",
    "\n",
    "numeric_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "categoric_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "# then we put the features list and the transformers together\n",
    "# using the column transformer\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric_imputer', numeric_imputer, features_numeric),\n",
    "    ('categoric_imputer', categoric_imputer, features_categoric)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see stratistics for frequent category imputer\n",
    "\n",
    "preprocessor.named_transformers_['categoric_imputer'].named_steps['imputer'].statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic best imputation method search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the preprocessing pipelines for both\n",
    "# numerical and categorical data\n",
    "\n",
    "# adapted from Scikit-learn code available here under BSD3 license:\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_transformer, features_numerical),\n",
    "        ('categorical', categorical_transformer, features_categorical)])\n",
    "\n",
    "# Note that to initialise the pipeline I pass any argument to the transformers.\n",
    "# Those will be changed during the gridsearch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', Lasso(max_iter=2000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create the grid with all the parameters that we would like to test\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__numerical__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__categorical__imputer__strategy': ['most_frequent', 'constant'],\n",
    "    'classifier__alpha': [10, 100, 200],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, iid=False, n_jobs=-1, scoring='r2')\n",
    "\n",
    "# cv=3 is the cross-validation\n",
    "# no_jobs =-1 indicates to use all available cpus\n",
    "# scoring='r2' indicates to evaluate using the r squared\n",
    "\n",
    "# for more details in the grid parameters visit:\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we train over all the possible combinations of the parameters above\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# and we print the best score over the train set\n",
    "print((\"best linear regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and find the best fit parameters like this\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally let's check the performance over the test set ==> overfitting\n",
    "print((\"best linear regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feature-engine\n",
    "from feature_engine import missing_data_imputers as mdi\n",
    "\n",
    "#Feature-Engine captures the numerical variables automatically\n",
    "# we call the imputer from feature-engine\n",
    "# we specify the imputation strategy, median in this case\n",
    "\n",
    "imputer = mdi.MeanMedianImputer(imputation_method='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the imputer\n",
    "\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the imputer found the numerical variables to\n",
    "# impute with the mean\n",
    "imputer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see the mean assigned to each variable\n",
    "imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-engine returns a dataframe\n",
    "\n",
    "tmp = imputer.transform(X_train)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that the numerical variables don't\n",
    "# contain NA any more\n",
    "\n",
    "tmp[imputer.variables].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to specify variables for specific technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do mean imputation this time\n",
    "# and let's do it over 2 of the 3 numerical variables\n",
    "\n",
    "imputer = mdi.MeanMedianImputer(imputation_method='mean',\n",
    "                                variables=['LotFrontage', 'MasVnrArea'])\n",
    "\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipeline from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('median_imputer', mdi.MeanMedianImputer(imputation_method='median',\n",
    "                                             variables = ['LotFrontage', 'GarageYrBlt'])),\n",
    "     \n",
    "    ('mean_imputer', mdi.MeanMedianImputer(imputation_method='mean',\n",
    "                                          variables = ['MasVnrArea'])),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform the data with the pipeline\n",
    "tmp = pipe.transform(X_train)\n",
    "\n",
    "# let's check null values are gone\n",
    "tmp.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbitrary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('imputer_999', mdi.ArbitraryNumberImputer(arbitrary_number = -999,\n",
    "                                             variables = ['LotFrontage', 'MasVnrArea'])),\n",
    "     \n",
    "    ('imputer_minus1', mdi.ArbitraryNumberImputer(arbitrary_number = -1,\n",
    "                                          variables = ['GarageYrBlt'])),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EndTail method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('imputer_skewed', mdi.EndTailImputer(distribution='skewed', tail='right',\n",
    "                                          variables=['GarageYrBlt', 'MasVnrArea'])),\n",
    "\n",
    "    ('imputer_gaussian', mdi.EndTailImputer(distribution='gaussian', tail='right',\n",
    "                                            variables=['LotFrontage'])),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add MissingIndicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('missing_ind', mdi.AddMissingIndicator()),\n",
    "    \n",
    "    ('imputer_mode', mdi.CategoricalVariableImputer(\n",
    "        imputation_method='frequent', variables=['FireplaceQu', 'BsmtQual'])),\n",
    "    \n",
    "    ('imputer_median', mdi.MeanMedianImputer(imputation_method='median',\n",
    "                                             variables=['LotFrontage', 'MasVnrArea', 'GarageYrBlt'])),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "tmp = pd.get_dummies(X_train)\n",
    "# The limitations:\n",
    "\n",
    "#The train set contains 13 dummy features, whereas the test set contains 12 features. This occurred because there was no category T in cabin in the test set.\n",
    "\n",
    "#This will cause problems if training and scoring models with scikit-learn, because predictors require train and test sets to be of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn. Limitations: returns an array and no variable names\n",
    "# we create and train the encoder\n",
    "encoder = OneHotEncoder(categories='auto',\n",
    "                       drop='first', # to return k-1, use drop=false to return k dummies\n",
    "                       sparse=False,\n",
    "                       handle_unknown='error') # helps deal with rare labels\n",
    "\n",
    "encoder.fit(X_train.fillna('Missing'))\n",
    "\n",
    "# transform the train set\n",
    "tmp = encoder.transform(X_train.fillna('Missing'))\n",
    "\n",
    "pd.DataFrame(tmp).head()\n",
    "\n",
    "# NEW: in latest release of Scikit-learn\n",
    "# we can now retrieve the feature names as follows:\n",
    "\n",
    "encoder.get_feature_names()\n",
    "\n",
    "# we can go ahead and transfom the test set\n",
    "# and then reconstitute it back to a pandas dataframe\n",
    "# and add the feature names derived by OHE\n",
    "\n",
    "tmp = encoder.transform(X_test.fillna('Missing'))\n",
    "\n",
    "tmp = pd.DataFrame(tmp)\n",
    "tmp.columns = encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureEngine. BEST. Can choose the variables to one-hot encode\n",
    "ohe_enc = OneHotCategoricalEncoder(\n",
    "    top_categories=None,\n",
    "    variables=['sex', 'embarked'], # we can select which variables to encode\n",
    "    drop_last=True) # to return k-1, false to return k\n",
    "\n",
    "#fit\n",
    "ohe_enc.fit(X_train.fillna('Missing'))\n",
    "#transform\n",
    "tmp = ohe_enc.transform(X_train.fillna('Missing'))\n",
    "\n",
    "# Note how feature-engine returns the dummy variables with their names, and drops the original variable, \n",
    "# leaving the dataset ready for further exploration or building machine learning models.\n",
    "\n",
    "# Note how this encoder returns a variable cabin_T for the test set as well, even though this category is not \n",
    "# present in the test set. This allows the integration with Scikit-learn pipeline and scoring of test set by \n",
    "# the built algorithm.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of top categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cardinality of each variables\n",
    "\n",
    "for col in data.columns:\n",
    "    print(col, ': ', len(data[col].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for high cardinal vars. better performance. Use feature-engine to keep the info of the ignored labels\n",
    "ohe_enc = OneHotCategoricalEncoder(\n",
    "    top_categories=10,  # you can change this value to select more or less variables\n",
    "    # we can select which variables to encode\n",
    "    variables=['Neighborhood', 'Exterior1st', 'Exterior2nd'],\n",
    "    drop_last=False)\n",
    "\n",
    "# fit\n",
    "ohe_enc.fit(X_train)\n",
    "\n",
    "# in the encoder dict we can observe each of the top categories\n",
    "# selected for each of the variables\n",
    "\n",
    "ohe_enc.encoder_dict_\n",
    "\n",
    "#transform on both train and test sets\n",
    "X_train = ohe_enc.transform(X_train)\n",
    "X_test = ohe_enc.transform(X_test)\n",
    "\n",
    "# let's explore the result\n",
    "X_train.head()\n",
    "\n",
    "\n",
    "#**Note**\n",
    "\n",
    "#If the argument variables is left to None, then the encoder will automatically identify all categorical variables\n",
    "\n",
    "#The encoder will not encode numerical variables. So if some of your numerical variables are in fact categories, you will need to re-cast them as object before using the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using feature-engine\n",
    "ordinal_enc = OrdinalCategoricalEncoder(\n",
    "    encoding_method='arbitrary',\n",
    "    variables=['Neighborhood', 'Exterior1st', 'Exterior2nd'])\n",
    "#fit\n",
    "ordinal_enc.fit(X_train)\n",
    "\n",
    "# in the encoder dict we can observe the numbers\n",
    "# assigned to each category for all the indicated variables\n",
    "ordinal_enc.encoder_dict_\n",
    "\n",
    "# Note, if there is a variable in the test set, for which the encoder doesn't have a number to assigned\n",
    "# (the category was not seen in the train set), the encoder will return an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count/Frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using feature-engine. Pb : can lead to different labels have the same encoded value\n",
    "count_enc = CountFrequencyCategoricalEncoder(\n",
    "    encoding_method='count', # to do frequency ==> encoding_method='frequency'\n",
    "    variables=['Neighborhood', 'Exterior1st', 'Exterior2nd'])\n",
    "\n",
    "count_enc.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target guided encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_enc = OrdinalCategoricalEncoder(\n",
    "    # NOTE that we indicate ordered in the encoding_method, otherwise it assings numbers arbitrarily\n",
    "    encoding_method='ordered',\n",
    "    variables=['Neighborhood', 'Exterior1st', 'Exterior2nd'])\n",
    "\n",
    "# when fitting the transformer, we need to pass the target as well\n",
    "# just like with any Scikit-learn predictor class\n",
    "ordinal_enc.fit(X_train, y_train)\n",
    "\n",
    "# transform\n",
    "X_train = ordinal_enc.transform(X_train)\n",
    "X_test = ordinal_enc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using feature-engine\n",
    "mean_enc = MeanCategoricalEncoder(\n",
    "    variables=['cabin', 'sex', 'embarked'])\n",
    "\n",
    "# when fitting the transformer, we need to pass the target as well\n",
    "# just like with any Scikit-learn predictor class\n",
    "mean_enc.fit(X_train, y_train)\n",
    "\n",
    "#transform\n",
    "X_train = mean_enc.transform(X_train)\n",
    "X_test = mean_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability ratio encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for binary classification \n",
    "ratio_enc = WoERatioCategoricalEncoder(\n",
    "    encoding_method = 'ratio',\n",
    "    variables=['cabin', 'sex', 'embarked'])\n",
    "\n",
    "# If the probability of target = 0 is zero for any category, the encoder will raise an error as the division \n",
    "# by zero is not defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight of evidence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for binary classification\n",
    "woe_enc = WoERatioCategoricalEncoder(\n",
    "    encoding_method = 'woe',\n",
    "    variables=['cabin', 'sex', 'embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare encoding methods performance on RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OHE(df):\n",
    "\n",
    "    df_OHE = pd.concat(\n",
    "        [df[['pclass', 'age', 'sibsp', 'parch', 'fare']],\n",
    "         pd.get_dummies(df[['sex', 'cabin', 'embarked']], drop_first=True)],\n",
    "        axis=1)\n",
    "\n",
    "    return df_OHE\n",
    "\n",
    "\n",
    "X_train_OHE = get_OHE(X_train)\n",
    "X_test_OHE = get_OHE(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_to_ordered(df_train, df_test, y_train, y_test):\n",
    "\n",
    "    # make a temporary copy of the datasets\n",
    "    df_train_temp = pd.concat([df_train, y_train], axis=1).copy()\n",
    "    df_test_temp = pd.concat([df_test, y_test], axis=1).copy()\n",
    "\n",
    "    for col in ['sex', 'cabin', 'embarked']:\n",
    "\n",
    "        # order categories according to target mean\n",
    "        ordered_labels = df_train_temp.groupby(\n",
    "            [col])['survived'].mean().sort_values().index\n",
    "\n",
    "        # create the dictionary to map the ordered labels to an ordinal number\n",
    "        ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n",
    "\n",
    "        # remap the categories  to these ordinal numbers\n",
    "        df_train_temp[col] = df_train[col].map(ordinal_label)\n",
    "        df_test_temp[col] = df_test[col].map(ordinal_label)\n",
    "\n",
    "    # remove the target\n",
    "    df_train_temp.drop(['survived'], axis=1, inplace=True)\n",
    "    df_test_temp.drop(['survived'], axis=1, inplace=True)\n",
    "\n",
    "    return df_train_temp, df_test_temp\n",
    "\n",
    "\n",
    "X_train_ordered, X_test_ordered = categories_to_ordered(\n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build random forests and compare performance in train and test set\n",
    "\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=39, max_depth=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "run_randomForests(X_train_OHE, X_test_OHE, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered labels\n",
    "run_randomForests(X_train_ordered, X_test_ordered, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare label encoding: first find rare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_encoding(X_train, X_test, variable, tolerance):\n",
    "\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    # find the most frequent category\n",
    "    frequent_cat = find_non_rare_labels(X_train, variable, tolerance)\n",
    "\n",
    "    # re-group rare labels\n",
    "    X_train[variable] = np.where(X_train[variable].isin(\n",
    "        frequent_cat), X_train[variable], 'Rare')\n",
    "    \n",
    "    X_test[variable] = np.where(X_test[variable].isin(\n",
    "        frequent_cat), X_test[variable], 'Rare')\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.categorical_encoders import RareLabelCategoricalEncoder\n",
    "\n",
    "# let's divide into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1), # predictors\n",
    "    data.SalePrice, # target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Rare value encoder\n",
    "rare_encoder = RareLabelCategoricalEncoder(\n",
    "    tol=0.05,  # minimal percentage to be considered non-rare\n",
    "    n_categories=4, # minimal number of categories the variable should have to re-cgroup rare categories\n",
    "    variables=['Neighborhood', 'Exterior1st', 'Exterior2nd',\n",
    "               'MasVnrType', 'ExterQual', 'BsmtCond'] # variables to re-group\n",
    ")  \n",
    "\n",
    "# fit\n",
    "rare_encoder.fit(X_train.fillna('Missing'))\n",
    "\n",
    "# transform\n",
    "X_train = rare_encoder.transform(X_train.fillna('Missing'))\n",
    "X_test = rare_encoder.transform(X_test.fillna('Missing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histograms to have a quick look at the variable distribution\n",
    "# histogram and Q-Q plots\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "    \n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "diagnostic_plots(data, 'GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logarithmic transformation\n",
    "\n",
    "data['GrLivArea_log'] = np.log(data['GrLivArea'])\n",
    "\n",
    "diagnostic_plots(data, 'GrLivArea_log')\n",
    "\n",
    "\n",
    "### Square root transformation\n",
    "data['GrLivArea_sqr'] = data['GrLivArea']**(1/2) \n",
    "\n",
    "# np.power(data['GrLivArea'], 1/2), np.sqrt(data['GrLivArea'])\n",
    "\n",
    "diagnostic_plots(data, 'GrLivArea_sqr')\n",
    "\n",
    "\n",
    "### Exponential transformation\n",
    "data['GrLivArea_exp'] = data['GrLivArea']**(1/1.5) # you can vary the exponent as needed\n",
    "\n",
    "# np.power(data['GrLivArea'], any exponent we want)\n",
    "\n",
    "diagnostic_plots(data, 'GrLivArea_exp')\n",
    "\n",
    "\n",
    "\n",
    "### BOX-COX Transformation\n",
    "data['GrLivArea_boxcox'], param = stats.boxcox(data['GrLivArea']) \n",
    "\n",
    "print('Optimal λ: ', param)\n",
    "\n",
    "diagnostic_plots(data, 'GrLivArea_boxcox')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic transformer\n",
    "\n",
    "# create a log transformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log, validate=True)\n",
    "\n",
    "# transform all the numerical and positive variables. sklearn doesn't accept missing values as input so need fillna\n",
    "\n",
    "data_t = transformer.transform(data[cols].fillna(1))\n",
    "\n",
    "# Scikit-learn returns NumPy arrays, so capture in dataframe\n",
    "# note that Scikit-learn will return an array with\n",
    "# only the columns indicated in cols\n",
    "\n",
    "data_t = pd.DataFrame(data_t, columns = cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeo-Johnson\n",
    "\n",
    "# Yeo-Johnson is an adaptation of Box-Cox that can also be used in negative value variables\n",
    "\n",
    "# call the transformer\n",
    "transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "\n",
    "# learn the lambda from the train set\n",
    "transformer.fit(data[cols].fillna(1))\n",
    "\n",
    "# transform the data\n",
    "data_t = transformer.transform(data[cols].fillna(1))\n",
    "\n",
    "# capture data in a dataframe\n",
    "data_t = pd.DataFrame(data_t, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-width with feature-engine\n",
    "from feature_engine.discretisers import EqualWidthDiscretiser\n",
    "\n",
    "# Let's separate into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['age', 'fare']],\n",
    "    data['survived'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "# replace NA in both  train and test sets\n",
    "X_train['age'] = impute_na(data, 'age')\n",
    "X_test['age'] = impute_na(data, 'age')\n",
    "X_train['fare'] = impute_na(data, 'fare')\n",
    "X_test['fare'] = impute_na(data, 'fare')\n",
    "\n",
    "# with feature engine we can automate the process for many variables\n",
    "disc = EqualWidthDiscretiser(bins=10, variables = ['age', 'fare'])\n",
    "\n",
    "disc.fit(X_train)\n",
    "\n",
    "# in the binner dict, we can see the limits of the intervals. For age\n",
    "# the value increases aproximately 7 years from one bin to the next.\n",
    "# for fare it increases in around 50 dollars from one interval to the \n",
    "# next, but it increases always the same value, aka, same width.\n",
    "\n",
    "disc.binner_dict_\n",
    "\n",
    "# transform train and text\n",
    "train_t = disc.transform(X_train)\n",
    "test_t = disc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-frequency. Boundaries represent quantiles --> each interval has the same amount of observations. \n",
    "# Useful for skwed variables cuz it spreads the observations over the different intervals equally\n",
    "# The width of intervals is not the same --> the nb of observations matters\n",
    "# Same process than equal-width\n",
    "\n",
    "# with feature engine we can automate the process for many variables\n",
    "# in one line of code\n",
    "\n",
    "disc = EqualFrequencyDiscretiser(q=10, variables = ['age', 'fare'], return_object = True # True to return cat vars)\n",
    "\n",
    "disc.fit(X_train)\n",
    "                                 \n",
    "# transform train and text\n",
    "train_t = disc.transform(X_train)\n",
    "test_t = disc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore if the bins have a linear relationship\n",
    "# with the target:\n",
    "\n",
    "pd.concat([train_t, y_train], axis=1).groupby('age')['survived'].mean().plot()\n",
    "plt.ylabel('mean of survived')\n",
    "\n",
    "# Then use discreted variables as cat vars and ordinalcategorical to obtain a monotonic relationship (re-order the order of bins)\n",
    "enc = OrdinalCategoricalEncoder(encoding_method = 'ordered')\n",
    "\n",
    "enc.fit(train_t, y_train)\n",
    "\n",
    "train_t = enc.transform(train_t)\n",
    "test_t = enc.transform(test_t)\n",
    "\n",
    "# in the map, we map bin to position. Run this code to know the mapping (already done)\n",
    "enc.encoder_dict_\n",
    "\n",
    "# Check\n",
    "pd.concat([train_t, y_train], axis=1).groupby('age')['survived'].mean().plot()\n",
    "plt.ylabel('mean of survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization using decision tree: find the optimal bins and 2 steps in one : discretisation and encoding same time\n",
    "# Good cuz create a monotonic relationship\n",
    "from feature_engine.discretisers import DecisionTreeDiscretiser\n",
    "\n",
    "# set up the decision tree discretiser indicating:\n",
    "# cross-validation number (cv)\n",
    "# how to evaluate model performance (scoring)\n",
    "# the variables we want to discretise (variables)\n",
    "# whether it is a target for regression or classification\n",
    "# and the grid with the parameters we want to test\n",
    "\n",
    "treeDisc = DecisionTreeDiscretiser(cv=10, scoring='accuracy',\n",
    "                                   variables=['age', 'fare'],\n",
    "                                   regression=False,\n",
    "                                   param_grid={'max_depth': [1, 2, 3],\n",
    "                                              'min_samples_leaf':[10,4]})\n",
    "\n",
    "treeDisc.fit(X_train, y_train)\n",
    "\n",
    "# we can inspect the tree for age to find the best params\n",
    "treeDisc.binner_dict_['age'].best_params_\n",
    "\n",
    "# and the performance obtained on the train set while fitting: print the score (accuracy for the best tree)\n",
    "treeDisc.scores_dict_['age']\n",
    "\n",
    "# let's transform the data\n",
    "train_t = treeDisc.transform(X_train)\n",
    "test_t = treeDisc.transform(X_test)\n",
    "\n",
    "# let's inspect how many bins we found\n",
    "train_t['age'].unique()\n",
    "\n",
    "# monotonic relationship with target: train set\n",
    "pd.concat([train_t, y_train], axis=1).groupby(['age'])['survived'].mean().plot()\n",
    "plt.title('Monotonic relationship between discretised Age and target')\n",
    "plt.ylabel('Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers handling: only remove from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR : upper limit = Q3 + IQR*1.5 and lower limit = Q1 - IQR*1.5. NB : IQR = Q3 - Q1\n",
    "def find_skewed_boundaries(df, variable, distance):\n",
    "\n",
    "    # Let's calculate the boundaries outside which sit the outliers\n",
    "    # for skewed distributions\n",
    "\n",
    "    # distance passed as an argument, gives us the option to\n",
    "    # estimate 1.5 times or 3 times the IQR to calculate\n",
    "    # the boundaries.\n",
    "\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "\n",
    "    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
    "    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
    "\n",
    "    return upper_boundary, lower_boundary\n",
    "\n",
    "# find limits for variable RM\n",
    "RM_upper_limit, RM_lower_limit = find_skewed_boundaries(df, 'RM', 1.5)\n",
    "RM_upper_limit, RM_lower_limit\n",
    "\n",
    "# let's flag the outliers in the data set\n",
    "outliers_RM = np.where(df['RM'] > RM_upper_limit, True,\n",
    "                       np.where(df['RM'] < RM_lower_limit, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming (remove outliers)\n",
    "df_trimmed = df.loc[~outliers_RM, ]\n",
    "\n",
    "df.shape, df_trimmed.shape # Check shape to see how many outliers we removed. No more than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping (fix limit values)\n",
    "# Now let's replace the outliers by the maximum and minimum limit. Replace and not flag\n",
    "\n",
    "df['RM']= np.where(df['RM'] > RM_upper_limit, RM_upper_limit,\n",
    "                       np.where(df['RM'] < RM_lower_limit, RM_lower_limit, df['RM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features scaling: important except for trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# let's separate the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# standardisation: with the StandardScaler from sklearn\n",
    "\n",
    "# set up the scaler\n",
    "scaler = StandardScaler()\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# let's transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax. Sensitive to outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# set up the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scaling. Good if data shows outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# set up the scaler\n",
    "scaler = RobustScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Variables that contain number and strings but the value is number or string. \n",
    "# So we extract the num part and the cat part into two different vars\n",
    "# extract numerical part\n",
    "data['var_num'] = pd.to_numeric(data[\"mixed_var\"],\n",
    "                                              errors='coerce',\n",
    "                                              downcast='integer')\n",
    "\n",
    "# extract categorical part\n",
    "data['var_cat'] = np.where(data['var_num'].isnull(),\n",
    "                                           data['mixed_var'], \n",
    "                                           np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Variables whose values contain number and srings. Like ticket : 145C25RE\n",
    "# Same technique: extract num and cat parts. But not optimal strategy, be careful\n",
    "\n",
    "# let's extract the numerical and categorical part for ticket\n",
    "# the variable ticket is extremely dirty, so there is only so much that we\n",
    "# can do, but here are some ideas:\n",
    "\n",
    "# extract the last bit of ticket as number\n",
    "data['ticket_num'] = data['ticket'].apply(lambda s: s.split()[-1])\n",
    "data['ticket_num'] = pd.to_numeric(data['ticket_num'],\n",
    "                                   errors='coerce',\n",
    "                                   downcast='integer')\n",
    "\n",
    "# extract the first part of ticket as category\n",
    "data['ticket_cat'] = data['ticket'].apply(lambda s: s.split()[0])\n",
    "data['ticket_cat'] = np.where(data['ticket_cat'].str.isdigit(), np.nan,\n",
    "                              data['ticket_cat'])\n",
    "\n",
    "data[['ticket', 'ticket_num', 'ticket_cat']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and time variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Convert into date format (originally string format). 2 vars here\n",
    "data['issue_dt'] = pd.to_datetime(data['date_issued'])\n",
    "data['last_pymnt_dt'] = pd.to_datetime(data['date_last_payment'])\n",
    "\n",
    "\n",
    "# Extracting week of year from date, varies from 1 to 52\n",
    "data['issue_dt_week'] = data['issue_dt'].dt.week\n",
    "\n",
    "# Extracting month from date - 1 to 12\n",
    "data['issue_dt_month'] = data['issue_dt'].dt.month\n",
    "\n",
    "# Extract quarter from date variable - 1 to 4\n",
    "data['issue_dt_quarter'] = data['issue_dt'].dt.quarter\n",
    "\n",
    "# extract semester\n",
    "data['issue_dt_semester'] = np.where(data['issue_dt_quarter'].isin([1,2]), 1, 2)\n",
    "\n",
    "# extract year \n",
    "data['issue_dt_year'] = data['issue_dt'].dt.year\n",
    "\n",
    "# day - numeric from 1-31\n",
    "data['issue_dt_day'] = data['issue_dt'].dt.day\n",
    "\n",
    "# day of the week - from 0 to 6\n",
    "data['issue_dt_dayofweek'] = data['issue_dt'].dt.dayofweek\n",
    "\n",
    "# day of the week - name\n",
    "data['issue_dt_dayofweek'] = data['issue_dt'].dt.day_name()\n",
    "\n",
    "# isweekend. ?\n",
    "data['issue_dt_is_weekend'] = np.where(data['issue_dt_dayofweek'].isin(['Sunday', 'Saturday']), 1,0)\n",
    "\n",
    "# Calculate the delta between two dates\n",
    "(data['last_pymnt_dt'] - data['issue_dt']).dt.days\n",
    "\n",
    "# calculate number of months passed between 2 dates\n",
    "data['months_passed'] = (data['last_pymnt_dt'] - data['issue_dt']) / np.timedelta64(1, 'M')\n",
    "data['months_passed'] = np.round(data['months_passed'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Extract time in a datetime format\n",
    "df['time'] = df['date'].dt.time\n",
    "\n",
    "# Extrat hour, minute and second\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['min'] = df['date'].dt.minute\n",
    "df['sec'] = df['date'].dt.second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recapitulation: Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso # The model to use\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# for feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine import missing_data_imputers as mdi\n",
    "from feature_engine import discretisers as dsc\n",
    "from feature_engine import categorical_encoders as ce\n",
    "\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling a feature engineering pipeline\n",
    "# First, make a list of the num and cat variables\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "# list of variables that contain year information for num vars that are not treated as num\n",
    "year_vars = [var for var in numerical if 'Yr' in var or 'Year' in var]\n",
    "\n",
    "# find discrete variables : To identify discrete variables, select from all the numerical ones, \n",
    "# those that contain a finite and small number of distinct values.\n",
    "discrete = []\n",
    "\n",
    "for var in numerical:\n",
    "    if len(data[var].unique()) < 20 and var not in year_vars:\n",
    "        print(var, ' values: ', data[var].unique())\n",
    "        discrete.append(var)\n",
    "print()\n",
    "print('There are {} discrete variables'.format(len(discrete)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(['Id', 'SalePrice'], axis=1),\n",
    "                                                    data['SalePrice'],\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will treat discrete variables as if they were categorical\n",
    "# to treat discrete as categorical using Feature-engine\n",
    "# we need to re-cast them as object\n",
    "\n",
    "X_train[discrete] = X_train[discrete].astype('O')\n",
    "X_test[discrete] = X_test[discrete].astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "house_pipe = Pipeline([\n",
    "\n",
    "    # missing data imputation - section 4\n",
    "    ('missing_ind', mdi.AddNaNBinaryImputer(  # add missing indicator\n",
    "        variables=['LotFrontage', 'MasVnrArea',  'GarageYrBlt'])),\n",
    "    ('imputer_num', mdi.MeanMedianImputer(imputation_method='median',  # replace MV by the median\n",
    "                                          variables=['LotFrontage', 'MasVnrArea',  'GarageYrBlt'])),\n",
    "    ('imputer_cat', mdi.CategoricalVariableImputer(variables=categorical)),  # add string 'missing' to all cat var with MV\n",
    "\n",
    "    # categorical encoding - section 6\n",
    "    ('rare_label_enc', ce.RareLabelCategoricalEncoder(\n",
    "        tol=0.05, n_categories=6, variables=categorical+discrete)),\n",
    "    ('categorical_enc', ce.OrdinalCategoricalEncoder(\n",
    "        encoding_method='ordered', variables=categorical+discrete)),\n",
    "\n",
    "    # discretisation + encoding - section 8\n",
    "    ('discretisation', dsc.EqualFrequencyDiscretiser(\n",
    "        q=5, return_object=True, variables=numerical)), # Create 5 intervals\n",
    "    ('encoding', ce.OrdinalCategoricalEncoder(\n",
    "        encoding_method='ordered', variables=numerical)), # encode results intervals reorder to create monotonic\n",
    "\n",
    "    # feature Scaling - section 10\n",
    "    ('scaler', StandardScaler()), # scale cuz we use a linear model\n",
    "    \n",
    "    # regression\n",
    "    ('model', Lasso(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit the pipeline\n",
    "house_pipe.fit(X_train, y_train)\n",
    "\n",
    "# let's get the predictions\n",
    "X_train_preds = house_pipe.predict(X_train)\n",
    "X_test_preds = house_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model performance:\n",
    "\n",
    "print('train mse: {}'.format(mean_squared_error(y_train, X_train_preds)))\n",
    "print('train rmse: {}'.format(sqrt(mean_squared_error(y_train, X_train_preds))))\n",
    "print('train r2: {}'.format(r2_score(y_train, X_train_preds)))\n",
    "print()\n",
    "print('test mse: {}'.format(mean_squared_error(y_test, X_test_preds)))\n",
    "print('test rmse: {}'.format(sqrt(mean_squared_error(y_test, X_test_preds))))\n",
    "print('test r2: {}'.format(r2_score(y_test, X_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the importance of the features\n",
    "# the importance is given by the absolute value of the coefficient\n",
    "# assigned by the Lasso\n",
    "\n",
    "importance = pd.Series(np.abs(house_pipe.named_steps['lasso'].coef_))\n",
    "importance.index = list(final_columns)+['LotFrontage_na', 'MasVnrArea_na',  'GarageYrBlt_na']\n",
    "importance.sort_values(inplace=True, ascending=False)\n",
    "importance.plot.bar(figsize=(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "titanic_pipe = Pipeline([\n",
    "\n",
    "    # missing data imputation - section 4\n",
    "    ('imputer_num',\n",
    "     mdi.ArbitraryNumberImputer(arbitrary_number=-1,\n",
    "                                variables=['age', 'fare', 'cabin_num'])),  # impute with value -1\n",
    "    ('imputer_cat',\n",
    "     mdi.CategoricalVariableImputer(variables=['embarked', 'cabin_cat'])),\n",
    "\n",
    "    # categorical encoding - section 6\n",
    "    ('encoder_rare_label',\n",
    "     ce.RareLabelCategoricalEncoder(tol=0.01,\n",
    "                                    n_categories=2,\n",
    "                                    variables=['embarked', 'cabin_cat'])),\n",
    "    ('categorical_encoder',\n",
    "     ce.OrdinalCategoricalEncoder(encoding_method='ordered',\n",
    "                                  variables=['cabin_cat', 'sex', 'embarked'])),\n",
    "\n",
    "    # Gradient Boosted machine\n",
    "    ('gbm', GradientBoostingClassifier(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create the grid with all the parameters that we would like to test\n",
    "\n",
    "param_grid = {\n",
    "    # try different feature engineering parameters\n",
    "    'imputer_num__arbitrary_number': [-1, 99],\n",
    "    'encoder_rare_label__tol': [0.1, 0.2],  # Test different levels of rare level: 10% or 20%\n",
    "    'categorical_encoder__encoding_method': ['ordered', 'arbitrary'],\n",
    "    \n",
    "    # try different gradient boosted tree model paramenters\n",
    "    'gbm__max_depth': [None, 1, 3],\n",
    "}\n",
    "\n",
    "\n",
    "# now we set up the grid search with cross-validation\n",
    "grid_search = GridSearchCV(titanic_pipe, param_grid,\n",
    "                           cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "# cv=5 is the cross-validation steps\n",
    "# no_jobs =-1 indicates to use all available cpus\n",
    "# scoring='roc-auc' indicates to evaluate the model performance with the roc-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we train over all the possible combinations of the parameters above\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# and we print the best score over the train set\n",
    "print((\"best roc-auc from grid search: %.3f\"\n",
    "       % grid_search.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print the best estimator parameters like this\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and find the best fit parameters like this\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally let's check the performance over the test set\n",
    "print((\"best linear regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the importance of the features\n",
    "\n",
    "importance = pd.Series(grid_search.best_estimator_['gbm'].feature_importances_)\n",
    "importance.index = data.drop('survived', axis=1).columns\n",
    "importance.sort_values(inplace=True, ascending=False)\n",
    "importance.plot.bar(figsize=(12,6))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
