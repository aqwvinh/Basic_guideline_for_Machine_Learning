0) Spend time to study the data. Here are some advice:
- check the % that train data represent
- plot label distribution
- check features correlation
- check missing values
- convert strings into floats. Ex: gender, class etc 
- drop, convert and create new features --> feature engineering
- think as "observations then decisions"
- use pipelines to avoid data leakage (too much preprocessing before split data)

STEPS TO FOLLOW FOR CLASSICAL TASKS (Regression, Classification, Clustering)
1)IMPORTATION
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

# machine learning
from sklearn.linear_model import LogisticRegression, LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge
from sklearn.svm import SVC, LinearSVC, SVR
from sklearn.ensemble import RandomForestClassifier, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.kernel_ridge import KernelRidge

### Import dataset 
# from Kaggle
import os
os.environ['KAGGLE_USERNAME'] = "vinhnguyen94" # username from the json file
os.environ['KAGGLE_KEY'] = "ee939400b83c8ef989727a70f29fab39" # key from the json file
!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle. Get the API on the address link
train = pd.read_csv("train.csv", index_col = "chosen column)

# on Colab
from google.colab import files
uploaded = files.upload()
import io
train=pd.read_csv(io.BytesIO(uploaded['train.csv']))

# on Jupyter
On Mac: open new window terminal, slide the data and copy the path
train = pd.read_csv("/Users/vinhnguyen/Desktop/Kaggle/Titanic/train.csv")

train.describe() // train.describe(include = ['O']) if we want only the strings information. include = 'all' is possible

# Check if the target variable is normal, sometimes there can be very bad mistake that would ruin the model (like a negative price)

#check the numbers of samples and features
train.shape, test.shape

# To see how many unique values in a column
train["col"].nunique()

#Save the 'Id' column
train_ID = train['Id']
test_ID = test['Id']
#Now drop the  'Id' colum since it's unnecessary for the prediction process.
train.drop("Id", axis = 1, inplace = True)
test.drop("Id", axis = 1, inplace = True)

2) Understand the data
Select the most important features to check the outliers and to check multicorrelation (reduce nb of features)
# Heatmap to see how features are correlated
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)

# Heatmap to check the features that contribute the most to the target variable
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'Target')['Target'].index
cm = np.corrcoef(train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

# Plot train Target
import matplotlib.pyplot as plt
import seaborn as sns
sns.countplot(df_train.target)
plt.xlabel('Target')
plt.title('Target distribution')

# Spot outliers. Strategy: delete very bad outliers. not all outliers to have a model robust on outliers
fig, ax = plt.subplots()
feature = "TotalBsmtSF"
ax.scatter(x = train[feature], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel(feature, fontsize=13)
plt.show()
# Drop chosen outliers
train = train.drop(train[(train['TotalBsmtSF']>4000) & (train['SalePrice']<300000)].index)  # Example for the house pricing

# Multiple plots TO DO
g = sns.FacetGrid(train, col='y') # Initialize the FacetGrid with the data (train) and the target var
g.map(plt.hist, 'X', bins=20) # Call plotting function .map with interval of 20 (for age for ex)

# Correlation between Gender and Target using the mean
train[["Sex","Target"]].groupby(['Sex']).mean().sort_values(by= "Target", ascending = False)




3) DATA PREPROCESSING
To do data pre-processing on both train and test sets. But WORK ON THE TRAIN FIRST THEN APPLY on TEST

# Merge 2 dataframes using the same column
new_train = pd.merge(train, text1, on='id', how='inner')

# Reset the index
test.reset_index(drop=False, inplace=True)

# Concatenate train and test data to do the preprocessing once. Not the best strategy but faster
ntrain = train.shape[0] # Essential to re-split the data before modeling. shape[0] to have only the nb of rows
ntest = test.shape[0]
y_train = train.target.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['target'], axis=1, inplace=True)
print("all_data size is : {}".format(all_data.shape))

# Select rows that contain a value in the column "A" 
df[df['A'].str.contains("hello")]


# Delete only true duplicates
train = train.drop_duplicates("col", keep = "first")
or
duplicates = train[train.duplicated('text')]
duplicates = duplicates[['id', 'text', 'keyword', 'location', 'target']].groupby(['text'], as_index=False).count().sort_values(by='id', ascending=False)
true_duplicate = duplicates["id"] > 1
duplicates[true_duplicate]
index_false_duplicate = duplicates[duplicates["id"] == 1].index 
duplicates.drop(index_false_duplicate, inplace=True)

### MISSING VALUES
# Print only the columns with missing values and there corresponding %
total_miss = all_data.isnull().sum().sort_values(ascending=False)
percent_miss = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_miss, percent_miss], axis=1, keys=['Total_miss', 'Percent_miss'])
// Strategy: When lots of features, we can delete the features with high % of missing values (let's say 15%). Check if these variables are important
// It's better to drop variables with high % of missing values instead of replacing by "Unknown" cuz it would create low variance which is uniformative for ML models.  

# (Variante) Print only the columns with missing values
missing_val_count_by_column = (df.isnull().sum())
print(missing_val_count_by_column[missing_val_count_by_column > 0])  # Print only the columns with missing values

# (Variante) Count the number of NaN per column. If we don't have so much features
len(train) - train.count()

# Get names of columns with missing values
cols_with_missing = [col for col in train.columns
                     if train[col].isnull().any()]

# Get columns for categorical features (replace by the most frequent occurence) and numerical features (replace by the mean)
categorical_cols = [cname for cname in all_data.columns if
                    all_data[cname].nunique() < 10 and    #to limit the size of the encoding
                    all_data[cname].dtype == "object" and
                    all_data[cname].isnull().sum() > 0
                    ]
numerical_cols = [cname for cname in all_data.columns if 
                all_data[cname].dtype in ['int64', 'float64'] and 
                all_data[cname].isnull().sum() > 0
                ]
# Replace by the mean (num) and the most frequent values (cat). For num, not always by the mean (median or 0)
for feature in categorical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].dropna().mode()[0])

for feature in numerical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].mean())
  
# (Variante) Complete missing values by the median / "Unknown"
train['Age']=train['Age'].fillna(train['Age'].median() / "Unknown")

# (Variante) Complete missing values by the most frequent occurence (letter or numerical category). Ex: frequent port of embarkation
freq_port = train.Embarked.dropna().mode()[0]
train["Embarked"] = train["Embarked"].fillna(freq_port)

# (Variante) Complete missing values by using other correlated features instead of replacing by the mean of the missing variable
# Guess Age values using median values for Age across sets of Pclass and Gender feature combinations
guess_ages = np.zeros((2,3))  # (2,3) cuz Sex is 2 and Pclass is 3
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = df[(df['Sex'] == i) & \
                                  (df['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    df['Age'] = df['Age'].astype(int)
    
    
### Categorizing
# Create ranges of age for plotting
train['Age_bins']=pd.cut(x=train['Age'], bins=[0,19, 39, 59,79], labels=['0-10s','20-30s','40-50s','60-70s']) # number of labels should be one less than bins
or (BETTER)
train['AgeBand'] = pd.cut(train['Age'], 5) # Create 5 categories of Age. We can add labels if we want. Use qcut if to have balanced categories

# Group the AgeBand depending on the target variable
train.groupby(["Age_bins"]).mean()
or
train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)

pd.crosstab(train.Age_bins, train.Survived).plot(kind="bar") #Survived = target

# Convert age into fixed categories of Age after the pd.cut   
dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
dataset.loc[ dataset['Age'] > 64, 'Age'] = 4


### Creation of new feature. Some ideas
# TITANIC. Create another feature called IsAlone. Reminder: combine = [train, test]
for dataset in combine:
    dataset["IsAlone"]=0 #Creation of the new feature
    dataset.loc[dataset["FamilySize"] == 1, "IsAlone"] = 1 # 1 if the person is alone
    
# HOUSE PRICING. Create a variable for the total size of the house. Try to group correlated variables
all_data["tot_surface"] = all_data["TotalBsmtSF"] + all_data["1stFlrSF"] + all_data["2ndFlrSF"]

### TO GO FURTHER: CHECK NORMALITY. The variables should be normal
from scipy import stats
#histogram and normal probability plot
sns.distplot(y_train);
fig = plt.figure()
res = stats.probplot(y_train, plot=plt) #The variable has to follow the red line (that represents the normal distribution).
# In case of hyperbol (positive skewness), apply log
y_train = np.log(y_train)   #Recheck now. Be careful on the submission, we need to compute the exponential
and don't forget: y_pred = np.exp(y_pred)
// Skewness measures the lack of symmetry in data distribution. It has to be close to 0
// Kurtosis is used to measure outliers present in the distribution. It has to be less than 3.

### Encoding
BEST PRACTICE : MANUALLY ENCODE WHEN ORDER MATTERS AND ONE-HOT ENCODE WHEN NO HIERARCHY

# Converting numeric features to categorical features. Especially for the year or month --> need to create categories
from sklearn.preprocessing import LabelEncoder
str_cols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass']
for col in str_cols:
    all_data[col] = all_data[col].astype(str)
    le = LabelEncoder()
    arr = le.fit_transform(all_data[col])
    all_data[col] = arr

# Count the occurence for each value for categorical feature
train.col1.value_counts()

# Convert string categorical vars into numerical ordered manually. Important to do that manually cuz order matters (Ordinal encoding)
all_data['GarageQual'].unique()
all_data["GarageQual"]=all_data["GaraQual"].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}).astype(int)
# One-hot encoding for categorical features using pd.dummies
cat_df = all_data.select_dtypes(include=[np.object])  #Take all the categorical features (strings)
for col in cat_df.columns:
    cat_df = pd.concat([cat_df, pd.get_dummies(cat_df[col], drop_first=True)], axis=1)
    cat_df.drop(col, axis=1, inplace=True)
data_encoded = pd.concat([all_data.select_dtypes(include=[np.number]),     # Concat with the numerical features
    cat_df.drop(cat_df.select_dtypes(include=[np.object]), axis=1)], axis=1)   # Don't keep the original categorical features
data_encoded.head(2)

# One-hot encoding using pandas (easier)
all_data = pd.get_dummies(all_data)

# Encode with LabelEncoder. THE DIFFERENCE BETWEEN ONE-HOT AND LABEL ENCODING IS THAT ONE-HOT DOESNT HAVE HIERARCHY. 
from sklearn.preprocessing import LabelEncoder
# Select all cat feature to encode: variables with categories and a hierarchy 
cat_features = ['MSSubClass', 'MSZoning', 'Street',
       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',
       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',
       'OverallQual', 'OverallCond', 'RoofStyle',
       'RoofMatl', 'MasVnrType',
       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',
       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
       'Heating', 'HeatingQC',
       'CentralAir', 'Electrical',
       'KitchenQual',
       'Functional', 'Fireplaces', 'GarageType', 'GarageFinish',
      'GarageQual', 'GarageCond', 'PavedDrive',
       'SaleType',
       'SaleCondition']

for feature in cat_features:
  enc = LabelEncoder()
  all_data[feature] = enc.fit_transform(all_data[feature])   # Not the best way but i've to find better one. Not good to fit the encoder on test



4) Modeling

# Split train and test sets
train = all_data[:ntrain]
test = all_data[ntrain:]

# Summarize the X and y variables. Reminder: y_train can be normalized by the log function
X = train.drop("label", axis = 1)
y = train.label   # check if the target variable is normalized: y_train = np.log(y_train)
X_test = test
X.shape, y.shape, X_test.shape  # To check if the shapes are correct

When the dataset is big enough, no need to do cross-validation so we can split into train and valid
# split the dataset into training and validation datasets
from sklearn import model_selection
X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.25, random_state=0,
                                                   stratify=y) #random_state to have the same alea and strafify to have a balance in label

# Manually do the split
// train = 80% first rows, then valid = 10% and finally test = 10%. Order matters here
def get_data_splits(df, valid_fraction=0.1):
    valid_fraction = 0.1
    df_srt = df.sort_values('click_time') # Wanna order depending on click_time. Delete if order doesn't matter
    valid_rows = int(len(df_srt) * valid_fraction)
    train = df_srt[:-valid_rows * 2]  # 80%
    # valid size == test size, last two sections of the data
    valid = df_srt[-valid_rows * 2:-valid_rows]  # 10%
    test = df_srt[-valid_rows:]  # 10%
    
    return train, valid, test
    
// to call the function: 
train, valid, test = get_data_splits(df)

# Using Robust Scaler to transform X_train. Aim: scale features to be robust to outliers
from sklearn.preprocessing import RobustScaler
robust_scaler = RobustScaler()
X_train_scaled = robust_scaler.fit_transform(X)
X_test_scaled = robust_scaler.transform(test)

# Creating the models. ADAPT ACCORDING THE TASK. HERE IS HOUSE PRICING PREDICTION
models = [LinearRegression(), SVR(), SGDRegressor(), SGDRegressor(max_iter=1000, tol=1e-3), GradientBoostingRegressor(), RandomForestRegressor(),
             Lasso(), Lasso(alpha=0.01, max_iter=10000), Ridge(), BayesianRidge(), KernelRidge(), KernelRidge(alpha=0.6, kernel='polynomial',degree=2, coef0=2.5),
             ElasticNet(), ElasticNet(alpha=0.001, max_iter=10000), ExtraTreesRegressor()]
names = ['Linear Regression','Support Vector Regression','Stochastic Gradient Descent','Stochastic Gradient Descent 2','Gradient Boosting Tree','Random Forest',
         'Lasso Regression','Lasso Regression 2','Ridge Regression','Bayesian Ridge Regression','Kernel Ridge Regression','Kernel Ridge Regression 2',
         'Elastic Net Regularization','Elastic Net Regularization 2','Extra Trees Regression']

# Define a root mean square error function
from sklearn.model_selection import KFold, cross_val_score
def rmse(model, X, y):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse
    
# Evaluate the train prediction. Useful when we average the predictions from several models
from sklearn.metrics import mean_squared_error
def rmsle(y_train, y_pred_train):
    return np.sqrt(mean_squared_error(y_train, y_pred_train))
print(rmsle(y_train, y_pred_train))


# Perform 5-folds cross-validation to evaluate the models 
for model, name in zip(models, names):
    # Root mean square error
    score = rmse(model, X_train_scaled, y)
    print(f"- {name}: Mean: {round(score.mean(),3)}, Std: {round(score.std(),3)}")
    
# (Variante) When no cross-validation and with accuracy metrics
for model, name in zip(models,names):
  score = train_model(model, X_train, y_train, X_valid, y_valid)
  print(f"- {name}: Accuracy: {round(score,3)}")


# (Variante) Model building. Following function is used to train a model and to compute the accuracy score
from sklearn import metrics
def train_model(classifier, train_vector, train_labels, valid_vector, valid_labels, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(train_vector, train_labels)
    
    # predict the labels on validation dataset
    predictions = classifier.predict(valid_vector)
    
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    
    return metrics.accuracy_score(predictions, valid_labels)
    
# Training with log reg
from sklearn import linear_model
accuracy = train_model(linear_model.LogisticRegression(random_state=0, penalty = "l2", solver='newton-cg', max_iter =200, C=1.6), X_train, y_train, X_valid, y_valid)
print ("LR : ", accuracy) 

### Tuning hyperparameters
# Gridsearch for log reg
from sklearn import model_selection
import math
param_grid = {'C': [0.1,1,10],
              'solver': ['newton-cg', 'saga', 'lbfgs']
              } 
 
clf = model_selection.GridSearchCV(linear_model.LogisticRegression(max_iter=300), param_grid = param_grid, scoring = "accuracy", cv = 5)
clf.fit(X_train, y_train)
print("The best parameters are: ",clf.best_params_)
print("The best score achieved is: ",math.sqrt(-clf.best_score_))


### Final model
best_model = ExtraTreesRegressor()  # For example with the best_params found earlier if applicable
best_model.fit(X_train_scaled, y_train)

# Predict on test
y_pred = best_model.predict(X_test_scaled)
or
y_pred = np.exp(best_model.predict(X_test_scaled))  # If we've applied log to normalize the variables
or
y_pred = le.inverse_transform(best_model.predict(X_test_scaled))  # If we have encoded the label before


# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': X_test.index,
                       'Target': y_pred})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")

# Submission on Jupyter if we wanna select the file
test.to_csv('/Users/vinhnguyen/Desktop/Kaggle/Titanic/submission.csv', index=False)



### STACKING MODELS
# Averaging base models. We can choose any model to add to the averaged_models
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):       #Create a class to extent scikit with our models
    def __init__(self, models):
        self.models = models
        
    # we define clones of the original models to fit the data in
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)

        return self
    
    #Now we do the predictions for cloned models and average them
    def predict(self, X):
        predictions = np.column_stack([
            model.predict(X) for model in self.models_
        ])
        return np.mean(predictions, axis=1)  
        
# We average the following models
ENet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)
GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)
KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
Lasso = Lasso(alpha =0.0005, random_state=1)

averaged_models = AveragingModels(models = (ENet, GBoost, KRR, Lasso))
score = rmse(averaged_models, X_train_scaled, y)
print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))


# Better results when adding a meta-learner    TO FURTHER UNDERSTAND
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # Train cloned base models then create out-of-fold predictions
        # that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
          for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self
   
    #Do the predictions of all base models on the test data and use the averaged predictions as 
    #meta-features for the final prediction which is done by the meta-model
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)

#Check if our meta model helps
stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),
                                                 meta_model = lasso)
score = rmse(stacked_averaged_models, X_train_scaled, y)
print("Stacking Averaged models score: {:.4f} ({:.4f})".format(score.mean(), score.std()))


### We can add XGBoost and LightGBM to the StackedRegressor defined previously. Strategy: compute prediction for the 3 models and do the weighted average
# Prediction from the averaged model
stacked_averaged_models.fit(X_train_scaled, y)
y_avg_pred = np.exp(stacked_averaged_models.predict(X_test_scaled))

# Prediction from the XGBoost
import xgboost as xgb
model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)

model_xgb.fit(X_train_scaled, y)
y_xgb_pred = np.exp(model_xgb.predict(X_test_scaled))

# Prediction from the LightGBM
import lightgbm as lgb
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

model_lgb.fit(X_train_scaled, y)
y_lgb_pred = np.exp(model_lgb.predict(X_test_scaled))

# Average the predictions
ensemble = y_avg_pred*0.70 + y_xgb_pred*0.15 + y_lgb_pred*0.15
ensemble

# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': test_ID,
                       'SalePrice': ensemble})
output.to_csv('submission.csv', index=False)

print("Your submission was successfully saved!")
