PERSONAL TIPS THAT I MET DURING DS PROJECTS

1)IMPORTATION
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd
pd.set_option('display.max_columns', None) # To show all columns

## Connect to SQL database and make some HTTP requests using authentification token using requests library
# Get token. 1-day period
url_token = 'API_token'
credentials = {'mail': 'user_mail,
         'password': 'user_password'}
res = requests.post(url_token, data = credentials)
token = res.json()['token']

# Get housing with the token and transform to DataFrame to be able to manipulate data in Jupyter Notebook. Get specific columns, not all columns in database
url_housing = 'API_housing?columns=host_id;housing_id;housing_name' 
headers = {'Authorization': f'Bearer {token}'}  # Need to use authorization header in every requests
housing_req = requests.get(url_housing, headers=headers)
housing = housing_req.json()
housing = pd.DataFrame(housing)

# Put new housing neighborhood (modify value in directly database)
for id in update_housing.housing_id:
    url_housing_put = f'API_housing/{id}'
    param = {'neighborhood': f'{update_housing.loc[update_housing.housing_id == id].neighborhood.values[0]}'}
    housing_put = requests.put(url_housing_put, data = param, headers = headers)

# Date
from datetime import datetime, date, timedelta
import calendar

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

Import Datasets 
# from Kaggle
import os
os.environ['KAGGLE_USERNAME'] = "vinhnguyen94" # username from the json file
os.environ['KAGGLE_KEY'] = "ee939400b83c8ef989727a70f29fab39" # key from the json file
!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle. Get the API on the address link
train = pd.read_csv("train.csv", index_col = "chosen column)
# on Colab
from google.colab import files
uploaded = files.upload()
import io
train=pd.read_csv(io.BytesIO(uploaded['train.csv']))
# on Jupyter
ttrain = pd.read_csv('../Datasets/train.csv')

train.describe() // train.describe(include = ['O']) if we want only the strings information. include = 'all' is possible

# Check if the target variable is normal, sometimes there can be very bad mistake that would ruin the model (like a negative price)

train["col"].nunique() # To see how many unique values in a column
train.value_counts()  # To see the values distribution


2) UNDERSTAND DATA
Select the most important features to check the outliers and to check multicorrelation (reduce nb of features)
# Heatmap to see how features are correlated
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)

# Heatmap to check the features that contribute the most to the target variable
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'Target')['Target'].index
cm = np.corrcoef(train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

# Plot train Target distribution
sns.countplot(df_train.target)
plt.xlabel('Target')
plt.title('Target distribution')

# Spot outliers. Strategy: delete very bad outliers. not all outliers to have a model robust on outliers
fig, ax = plt.subplots()
feature = "TotalBsmtSF"
target = 'SalePrice'
ax.scatter(x = train[feature], y = train[target])
plt.ylabel(target, fontsize=13)
plt.xlabel(feature, fontsize=13)
plt.show()
# Drop chosen outliers
train = train.drop(train[(train[feature]>4000) & (train[target]<300000)].index)  # Example for the house pricing

# Multiple plots TO COMPLETE
g = sns.FacetGrid(train, col='y') # Initialize the FacetGrid with the data (train) and the target var
g.map(plt.hist, 'X', bins=20) # Call plotting function .map with interval of 20 (for age for ex)

# Groupby using different aggregation methods: mean and sum
df_quartier = df.groupby(['region','quartier']).agg(revenue_brut_sum = ('revenue_brut_year', 'sum'),
                                                    revenue_brut_night_mean = ('revenue_brut_moyen_night', 'mean')).reset_index()

# Move specific col to specific place
y_name = 'arnd'
y = df.pop(y_name)
df.insert(1, y_name, y). # Put at 2nd column


3) DATA PREPROCESSING
To do data pre-processing on both train and test sets. But WORK ON THE TRAIN FIRST THEN APPLY on TEST

# CONCATENATE train and test set to apply some data preprocessing
combine=pd.concat([train,test])
# MERGE 2 dataframes using the same column
new_train = pd.merge(train, text1, on='id', how='outer')
# MAP one column but need unique index
housing['quartier'] = housing.housing_id.map(df_quartier.set_index('housing_id')['quartier'])

# CREATE NEW COLUMN ON CONDITION
conditions = [(housing.postal_code.str.startswith('75')),(housing.postal_code.str.startswith(('78','91', '92', '93', '94', '95')))]
values = ['paris', 'banlieue']
housing['region'] = np.select(conditions, values)

# Extract first n characters from left of columns in pandas
df['zipcode'].str[:5]

# Keep only rows whose strings startwiths '75'
df_paris = df.loc[df.zipcode.str.startswith('75', na=False)]

# Set max value in a column to 30
maxVal = 30
df['col'] = df['col'].where(df['col'] <= maxVal, maxVal)


# Select rows that contain a value in a specific column
df[df['col'].str.contains("hello")]

# Delete only true duplicates
train = train.drop_duplicates("col", keep = "first")
or PERSONALIZED 
duplicates = train[train.duplicated('text')]
duplicates = duplicates[['id', 'text', 'keyword', 'location', 'target']].groupby(['text'], as_index=False).count().sort_values(by='id', ascending=False)
true_duplicate = duplicates["id"] > 1. # Can adapt
duplicates[true_duplicate]
index_false_duplicate = duplicates[duplicates["id"] == 1].index 
duplicates.drop(index_false_duplicate, inplace=True)

# Duplicates that are differently labeled
df_train['target_relabeled'] = df_train['target'].copy() 
df_train.loc[df_train['text'] == 'original text', 'target_relabeled'] = 0/1 #Choose the correct label

# Create column with tuple. Useful for GPS using lat and lnt
housing['GPS'] = tuple(zip(housing.lat, housing.lng))

# CALCULATE DISTANCE (haversine distance)
pip install haversine # install module
from haversine import haversine
name = quartiers_paris.L_QU.unique() # Distance of each house from every quarters in Paris
for q,n in zip(quartiers_paris.GPS, name):
  distances = []
  for h in housing_paris.GPS:
    distances.append(haversine(h,q))
  housing_paris[n] = distances
  
# Create function assign_quartier
def assign_quartier(df):
  min_val_col_name = df.idxmin(axis=1). # Take the quartier with the min distance
  df['quartier'] = min_val_col_name
  
# Keep rows with specific values in a specific column  
new_amenities = new_amenities.loc[new_amenities.title.isin(selected_amenities)] # Set selected_amenities list before

# Check amenities distribution (multiple items per row that are separated by ",")
pd.Series(np.concatenate(listings['amenities'].map(lambda x: x.split(",")))).value_counts()
# If multiple items are separated by other characters, we can create a cleaning function and use it with apply
def split_amenities(amenities):
    amenity_list = amenities \
        .replace("\"", "")  \
        .replace("{", "")  \
        .replace("}", "")  \
        .lower().split(",")
    return amenity_list
    
listings['amenities'] = listings['amenities'].apply(split_amenities)

# Convert date to day of week name
df['date'] = pd.to_datetime(df['date'])
df['weekday'] = df['date'].dt.weekday
df['weekday_name'] = df['date'].dt.day_name()

### MISSING VALUES
# Print only the columns with missing values and their corresponding %
df_mv = reservation2
total_miss = df_mv.isnull().sum().sort_values(ascending=False)
percent_miss = (df_mv.isnull().sum()/df_mv.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_miss, percent_miss], axis=1, keys=['Total_miss', 'Percent_miss'])
missing_data[missing_data.Percent_miss > 0]
THEN
mv_features = ['tax', 'fees']
for f in mv_features:
    reservation2[f] = reservation2[f].fillna(0)
    
    
# Replace MV for one col depending on another column. Here replace MV "beds" by "accomodates"/2 if > 1 else 1
c1 = (listings.accommodates > 1)
c2 = (listings.accommodates <= 1)
listings.loc[c1,'beds'] = listings.loc[c1,'beds'].fillna(listings.loc[c1,'accommodates']/2)
listings.loc[c2,'beds'] = listings.loc[c2,'beds'].fillna(1)

# (Variante) Print only the columns with missing values
missing_val_count_by_column = (df.isnull().sum())
print(missing_val_count_by_column[missing_val_count_by_column > 0])  # Print only the columns with missing values
# (Variante) Count the number of NaN per column. If we don't have so much features
len(train) - train.count()

# Get names of columns with missing values
cols_with_missing = [col for col in train.columns
                     if train[col].isnull().any()]

# Get columns for categorical features (replace by the most frequent occurence) and numerical features (replace by the mean)
categorical_cols = [cname for cname in all_data.columns if
                    all_data[cname].nunique() < 10 and    #to limit the size of the encoding
                    all_data[cname].dtype == "object" and
                    all_data[cname].isnull().sum() > 0
                    ]
numerical_cols = [cname for cname in all_data.columns if 
                all_data[cname].dtype in ['int64', 'float64'] and 
                all_data[cname].isnull().sum() > 0
                ]
# Replace by the mean (num) and the most frequent values (cat). For num, not always by the mean (median or 0)
for feature in categorical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].dropna().mode()[0])

for feature in numerical_cols:
  all_data[feature] = all_data[feature].fillna(all_data[feature].mean())
  
# (Variante) Complete missing values by the median / "Unknown"
train['Age']=train['Age'].fillna(train['Age'].median() / "Unknown")

# (Variante) Complete missing values by the most frequent occurence (letter or numerical category). Ex: frequent port of embarkation
freq_port = train.Embarked.dropna().mode()[0]
train["Embarked"] = train["Embarked"].fillna(freq_port)

# (Variante) Complete missing values by using other correlated features instead of replacing by the mean of the missing variable
# Guess Age values using median values for Age across sets of Pclass and Gender feature combinations
guess_ages = np.zeros((2,3))  # (2,3) cuz Sex is 2 and Pclass is 3
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = df[(df['Sex'] == i) & \
                                  (df['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    df['Age'] = df['Age'].astype(int)

# Drop na from specific column
df = df.dropna(subset = ['colA'])

# Replace date for specific row
reservation.at[reservation.loc[reservation.code == 'resa_code'].index, 'departure_date']= pd.Timestamp('2019-12-31')

# Take the date of the first resa 
reservation['first_resa'] = reservation.groupby(['housing_id', 'housing_name'])['arrival_date'].transform("min") # Add column first_resa with the earliest resa per housing
 
 
# Split resa per day: have one row per day of reservation
reservation_day0 = reservation2[['id', 'arrival_date', 'departure_date']]
reservation_day0 = pd.melt(reservation_day0, id_vars='id', value_name='date')
reservation_day0.date = pd.to_datetime(reservation_day0.date)
reservation_day0.set_index('date', inplace=True)
reservation_day0.drop('variable', axis=1, inplace=True)
reservation_day = reservation_day0.groupby('id').resample('D').ffill().reset_index(level=0, drop=True).reset_index().  # Resample per day
reservation_day = pd.merge(reservation2, reservation_day)
# Drop latest day per resa because we only count the nb of nights and not days
reservation_day= reservation_day.drop(reservation_day[reservation_day['departure_date'] == reservation_day['date']].index)
 
 
## CREATE FUNCTION with if condition
def is_studio_price(bedroom_count, nightPrice):
  is_studio = 1 if bedroom_count == 0 else 0
  if is_studio ==1:
    nightPrice *= 0.10 # Example: decrease nightPrice by 90% if it is a studio
  return nightPrice
 
### Categorizing
# Create ranges of age for plotting
train['Age_bins']=pd.cut(x=train['Age'], bins=[0,19, 39, 59,79], labels=['0-10s','20-30s','40-50s','60-70s']) # number of labels should be one less than bins
or (BETTER)
train['AgeBand'] = pd.cut(train['Age'], 5) # Create 5 categories of Age. We can add labels if we want. Use qcut if to have balanced categories


pd.crosstab(train.Age_bins, train.Survived).plot(kind="bar") #Survived = target

# Convert age into fixed categories of Age after the pd.cut   
dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
dataset.loc[ dataset['Age'] > 64, 'Age'] = 4


### Creation of new feature. Some ideas
# TITANIC. Create another feature called IsAlone. Reminder: combine = [train, test]
for dataset in combine:
    dataset["IsAlone"]=0 #Creation of the new feature
    dataset.loc[dataset["FamilySize"] == 1, "IsAlone"] = 1 # 1 if the person is alone
    
# HOUSE PRICING. Create a variable for the total size of the house. Try to group correlated variables
all_data["tot_surface"] = all_data["TotalBsmtSF"] + all_data["1stFlrSF"] + all_data["2ndFlrSF"]


### TO GO FURTHER: CHECK NORMALITY. The variables should be normal
from scipy import stats
#histogram and normal probability plot
sns.distplot(y_train);
fig = plt.figure()
res = stats.probplot(y_train, plot=plt) #The variable has to follow the red line (that represents the normal distribution).
# In case of hyperbol (positive skewness), apply log
y_train = np.log(y_train)   #Recheck now. Be careful on the submission, we need to compute the exponential
and don't forget: y_pred = np.exp(y_pred)
// Skewness measures the lack of symmetry in data distribution. It has to be close to 0
// Kurtosis is used to measure outliers present in the distribution. It has to be less than 3.

## Detect outliers
plt.boxplot(df['target'])
Q1 = df['target'].quantile(0.25)
Q3 = df['target'].quantile(0.75)
IQR = Q3 - Q1
print(IQR)
df['target'] < (Q1 - 1.5 * IQR)  #lower bound
df['target'] > (Q3 + 1.5 * IQR) #upper bound




### GRAPH
# Gmaps
conda install -c conda-forge gmapsimport gmaps
gmaps.configure(api_key='YourAPI')
fig = gmaps.figure()
heatmap_layer_inside = gmaps.heatmap_layer(
  df[['lat','lng']],
  weights=df[criterion]
)
fig.add_layer(heatmap_layer_inside)
fig

# DOUBLE LINE GRAPH
ax = df.plot(x = 'month', y= ['Y1', 'Y2'], figsize = (25,10), title = 'yourTitle')
_ = plt.xticks(fontsize = 16, rotation=45) # tip for changing the ticks

# Bar plot horizontal
ax = df.sort_values('revenue', ascending=True).plot.barh(x='quartier', y='revenue', figsize = (15,15), title = 'yourTitle')
# show text on top of bars (vertical) or after bars (horizontal)
bars = ax.patches
# Get labels to show
labels = round(df.revenue,0).astype(int)
for rect, label in zip(bars, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, label,
            ha='center', va='bottom')
  
# Bar plot from df with values as text on the graph  
ax = sns.barplot(x='category', y="price", data=df_plot)
for _, row in to_draw.iterrows(): #Add text on graph
    ax.text(row.name,row.price, round(row.price,2), color='black', ha="center")
ax.set_title('My_Title') # Add title

# COMBINED BAR AND LINE CHART (SEABORN)
fig, ax1 = plt.subplots(figsize=(20,9))
#bar plot creation
color1 = 'lightgray'
ax1.set_title('yourTitle', fontsize=24)
ax1.set_xlabel('X', fontsize=14)
ax1 = sns.barplot(x='X', y='Y', data = df_plot, color = color1)
ax1.set_ylabel('Y', fontsize=14, color = "black")
plt.xticks(rotation=45) # for better reading
#specify we want to share the same x-axis
ax2 = ax1.twinx()
#line plot creation
color2 = "green"
ax2.set_ylabel('Y2', fontsize=14, color = color2 )
ax2 = sns.lineplot(x=range(0,len(df_plot.Y2)), y='Y2', data = df_plot, sort=False, color=color2)
# Show yearly occ_rate
ax3 = ax2.twiny()
# line plot creation
color3 = 'navy'
ax3 = sns.lineplot(x=range(0,len(df_plot.Y3)), y='Y3', data=df_plot, sort=False, color = color3)
# Round values for axis
ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))
#show plot
plt.show()

# PIE CHART
labels = df.country_name
sizes = df.ratio
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()


# CONVERT PHONE NUMBER TO COUNTRY CODE
pip install phonenumbers
import phonenumbers
from phonenumbers.phonenumberutil import (
    region_code_for_country_code,
    region_code_for_number
)

country_code = []
phone_number = []
incorrect_number = []
def number_to_countryCode(df):
    for n in df.phone:
        try:
            pn = phonenumbers.parse(f'{n}')
            code = region_code_for_country_code(pn.country_code)
            phone_number.append(n)
            country_code.append(code)
        except:
            incorrect_number.append(n)
            

## DATA MANIPULATION
# Take value of a cell in a dataframe refering specific value in a column ==> we get the string and not the array
neighborhood = df[df.housing_id == id].neighborhood.values[0]

### ENCODING
BEST PRACTICE : MANUALLY ENCODE WHEN ORDER MATTERS AND ONE-HOT ENCODE WHEN NO HIERARCHY

# Manual encoding
housing.id = np.where(housing.id > 0, 1, 0)

# Converting numeric features to categorical features. Especially for the year or month --> need to create categories
from sklearn.preprocessing import LabelEncoder
str_cols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass']
for col in str_cols:
    all_data[col] = all_data[col].astype(str)
    le = LabelEncoder()
    arr = le.fit_transform(all_data[col])
    all_data[col] = arr

# Count the occurence for each value for categorical feature
train.col1.value_counts()

# Convert string categorical vars into numerical ordered manually. Important to do that manually cuz order matters (Ordinal encoding)
all_data['GarageQual'].unique()
all_data["GarageQual"]=all_data["GaraQual"].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}).astype(int)
# One-hot encoding for categorical features using pd.dummies
cat_df = all_data.select_dtypes(include=[np.object])  #Take all the categorical features (strings)
for col in cat_df.columns:
    cat_df = pd.concat([cat_df, pd.get_dummies(cat_df[col], drop_first=True)], axis=1)
    cat_df.drop(col, axis=1, inplace=True)
data_encoded = pd.concat([all_data.select_dtypes(include=[np.number]),     # Concat with the numerical features
    cat_df.drop(cat_df.select_dtypes(include=[np.object]), axis=1)], axis=1)   # Don't keep the original categorical features
data_encoded.head(2)

# One-hot encoding using pandas (easier)
all_data = pd.get_dummies(all_data)

# Encode with LabelEncoder. THE DIFFERENCE BETWEEN ONE-HOT AND LABEL ENCODING IS THAT ONE-HOT DOESNT HAVE HIERARCHY. 
from sklearn.preprocessing import LabelEncoder
# Select all cat feature to encode: variables with categories and a hierarchy 
cat_features = ['MSSubClass', 'MSZoning', 'Street',
       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',
       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',
       'OverallQual', 'OverallCond', 'RoofStyle',
       'RoofMatl', 'MasVnrType',
       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',
       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
       'Heating', 'HeatingQC',
       'CentralAir', 'Electrical',
       'KitchenQual',
       'Functional', 'Fireplaces', 'GarageType', 'GarageFinish',
      'GarageQual', 'GarageCond', 'PavedDrive',
       'SaleType',
       'SaleCondition']

for feature in cat_features:
  enc = LabelEncoder()
  all_data[feature] = enc.fit_transform(all_data[feature])   # Not the best way but i've to find better one. Not good to fit the encoder on test



GODD PRACTICE: USE A CLASS TRANSFORMER. ==> Define a class that does the data preprocessing on the train then on the test set



4) Modeling
# split the dataset into training and validation datasets
from sklearn import model_selection
X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.25, random_state=0,
                                                   stratify=y) #random_state to have the same alea and strafify to have a balance in label

TIPS : 
CROSS-VALIDATION ON THE TRAIN SET 
For small dataset (<100K rows is a small dataset) => cross-validation
How choose K ? Value of K shouldn’t be too small or too high, ideally 5 to 10 depending on the data size. 
         The higher value of K leads to less biased model (but large variance might lead to over-fit)
               
from sklearn.model_selection import cross_val_score
from sklearn import metrics  --> See list of metrics here : https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
# Example of 5-fold cross-validation with SVM Regressor
model = svm.SVR(kernel='linear', C=1, random_state=42)
scores = cross_val_score(model, X_train, y_train, cv=5, scoring = "neg_root_mean_squared_error") ==> "scores" gives an array with the multiple scores
# Give the mean score
print(f" {round(scores.mean(),2)} score with a standard deviation of {round(scores.std(),2)}"




# Summarize the X and y variables. Reminder: y_train can be normalized by the log function
X = train.drop("label", axis = 1)
y = train.label   # check if the target variable is normalized: y_train = np.log(y_train)
X_test = test
X.shape, y.shape, X_test.shape  # To check if the shapes are correct


# Manually do the split. NOT best practice
// train = 80% first rows, then valid = 10% and finally test = 10%. Order matters here
def get_data_splits(df, valid_fraction=0.1):
    valid_fraction = 0.1
    df_srt = df.sort_values('click_time') # Wanna order depending on click_time. Delete if order doesn't matter
    valid_rows = int(len(df_srt) * valid_fraction)
    train = df_srt[:-valid_rows * 2]  # 80%
    # valid size == test size, last two sections of the data
    valid = df_srt[-valid_rows * 2:-valid_rows]  # 10%
    test = df_srt[-valid_rows:]  # 10%
    
    return train, valid, test
    
// to call the function: 
train, valid, test = get_data_splits(df)

# Using Robust Scaler to transform X_train. Aim: scale features to be robust to outliers
from sklearn.preprocessing import RobustScaler
robust_scaler = RobustScaler()
X_train_scaled = robust_scaler.fit_transform(X)
X_test_scaled = robust_scaler.transform(test)

# Creating the models. ADAPT ACCORDING THE TASK. HERE IS HOUSE PRICING PREDICTION
models = [LinearRegression(), SVR(), SGDRegressor(), SGDRegressor(max_iter=1000, tol=1e-3), GradientBoostingRegressor(), RandomForestRegressor(),
             Lasso(), Lasso(alpha=0.01, max_iter=10000), Ridge(), BayesianRidge(), KernelRidge(), KernelRidge(alpha=0.6, kernel='polynomial',degree=2, coef0=2.5),
             ElasticNet(), ElasticNet(alpha=0.001, max_iter=10000), ExtraTreesRegressor()]
names = ['Linear Regression','Support Vector Regression','Stochastic Gradient Descent','Stochastic Gradient Descent 2','Gradient Boosting Tree','Random Forest',
         'Lasso Regression','Lasso Regression 2','Ridge Regression','Bayesian Ridge Regression','Kernel Ridge Regression','Kernel Ridge Regression 2',
         'Elastic Net Regularization','Elastic Net Regularization 2','Extra Trees Regression']

# Define a root mean square error function using cross-validation
from sklearn.model_selection import KFold, cross_val_score
def rmse(model, X, y):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse
 or
    accuracy = cross_val_score(model, X, y, scoring = "accuracy", cv=5) # for accuracy
 
 
# Evaluate the train prediction. Useful when we average the predictions from several models
from sklearn.metrics import mean_squared_error
def rmsle(y_train, y_pred_train):
    return np.sqrt(mean_squared_error(y_train, y_pred_train))
print(rmsle(y_train, y_pred_train))


# Perform 5-folds cross-validation to evaluate the models 
for model, name in zip(models, names):
    # Root mean square error
    score = rmse(model, X_train_scaled, y)
    print(f"- {name}: Mean: {round(score.mean(),3)}, Std: {round(score.std(),3)}")
    
# (Variante) When no cross-validation and with accuracy metrics
for model, name in zip(models,names):
  score = train_model(model, X_train, y_train, X_valid, y_valid)
  print(f"- {name}: Accuracy: {round(score,3)}")


# (Variante) Model building. Following function is used to train a model and to compute the accuracy score
from sklearn import metrics
def train_model(classifier, train_vector, train_labels, valid_vector, valid_labels, is_neural_net=False):
    # fit the training dataset on the classifier
    classifier.fit(train_vector, train_labels)
    
    # predict the labels on validation dataset
    predictions = classifier.predict(valid_vector)
    
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    
    return metrics.accuracy_score(predictions, valid_labels)
    
# Training with log reg
from sklearn import linear_model
accuracy = train_model(linear_model.LogisticRegression(random_state=0, penalty = "l2", solver='newton-cg', max_iter =200, C=1.6), X_train, y_train, X_valid, y_valid)
print ("LR : ", accuracy) 

### Tuning hyperparameters
# Gridsearch for log reg
from sklearn import model_selection
import math
param_grid = {'C': [0.1,1,10],
              'solver': ['newton-cg', 'saga', 'lbfgs']
              } 
 
clf = model_selection.GridSearchCV(linear_model.LogisticRegression(max_iter=300), param_grid = param_grid, scoring = "accuracy", cv = 5)
clf.fit(X_train, y_train)
print("The best parameters are: ",clf.best_params_)
print("The best score achieved is: ",math.sqrt(-clf.best_score_))


### Final model
best_model = ExtraTreesRegressor()  # For example with the best_params found earlier if applicable
best_model.fit(X_train_scaled, y_train)

# Predict on test
y_pred = best_model.predict(X_test_scaled)
or
y_pred = np.exp(best_model.predict(X_test_scaled))  # If we've applied log to normalize the variables
or
y_pred = le.inverse_transform(best_model.predict(X_test_scaled))  # If we have encoded the label before


# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': X_test.index,
                       'Target': y_pred})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")

# Submission on Jupyter if we wanna select the file
test.to_csv('/Users/vinhnguyen/Desktop/Kaggle/Titanic/submission.csv', index=False)



### STACKING MODELS
# Averaging base models. We can choose any model to add to the averaged_models
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):       #Create a class to extent scikit with our models
    def __init__(self, models):
        self.models = models
        
    # we define clones of the original models to fit the data in
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)

        return self
    
    #Now we do the predictions for cloned models and average them
    def predict(self, X):
        predictions = np.column_stack([
            model.predict(X) for model in self.models_
        ])
        return np.mean(predictions, axis=1)  
        
# We average the following models
ENet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)
GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)
KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
Lasso = Lasso(alpha =0.0005, random_state=1)

averaged_models = AveragingModels(models = (ENet, GBoost, KRR, Lasso))
score = rmse(averaged_models, X_train_scaled, y)
print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))


# Better results when adding a meta-learner    TO FURTHER UNDERSTAND
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # Train cloned base models then create out-of-fold predictions
        # that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
          for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self
   
    #Do the predictions of all base models on the test data and use the averaged predictions as 
    #meta-features for the final prediction which is done by the meta-model
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)

#Check if our meta model helps
stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),
                                                 meta_model = lasso)
score = rmse(stacked_averaged_models, X_train_scaled, y)
print("Stacking Averaged models score: {:.4f} ({:.4f})".format(score.mean(), score.std()))


### We can add XGBoost and LightGBM to the StackedRegressor defined previously. Strategy: compute prediction for the 3 models and do the weighted average
# Prediction from the averaged model
stacked_averaged_models.fit(X_train_scaled, y)
y_avg_pred = np.exp(stacked_averaged_models.predict(X_test_scaled))

# Prediction from the XGBoost
import xgboost as xgb
model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)

model_xgb.fit(X_train_scaled, y)
y_xgb_pred = np.exp(model_xgb.predict(X_test_scaled))

# Prediction from the LightGBM
import lightgbm as lgb
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

model_lgb.fit(X_train_scaled, y)
y_lgb_pred = np.exp(model_lgb.predict(X_test_scaled))

# Average the predictions
ensemble = y_avg_pred*0.70 + y_xgb_pred*0.15 + y_lgb_pred*0.15
ensemble

# Save predictions in format used for competition scoring
output = pd.DataFrame({'Id': test_ID,
                       'SalePrice': ensemble})
output.to_csv('submission.csv', index=False)

print("Your submission was successfully saved!")
